"""
web_crawler.py - Web é¡µé¢æŠ“å–å·¥å…·

åŠŸèƒ½ï¼š
- ä½¿ç”¨ Selenium æŠ“å–æ™®é€šç½‘é¡µå†…å®¹
- ç”Ÿæˆç½‘é¡µé•¿æˆªå›¾ (PNG) å’Œé«˜ä¿çœŸ PDF å­˜æ¡£
- è°ƒç”¨ LLM å¯¹æŠ“å–å†…å®¹è¿›è¡Œç»“æ„åŒ–æ•´ç†

ä¾èµ–ï¼šselenium, beautifulsoup4, openai, webdriver-manager
"""
import base64
from datetime import datetime
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import time
import re
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from common import setup_logger

logger = setup_logger("web_crawler")

# ================= é…ç½®åŒºåŸŸ =================
# è®¾ç½®æ™®é€š Web URL æŠ“å–æº
# é€‚ç”¨äºæ²¡æœ‰ RSS çš„å•é¡µé¢ï¼Œå¦‚å…·ä½“çš„ä¸€ç¯‡åšæ–‡æˆ–é™æ€é¡µé¢
web_sources = {
    # "databricks_ebook": "https://www.databricks.com/resources/ebook/state-of-ai-agents?utm_source=twitter&utm_medium=organic-social&utm_scid=701Vp00000V6YWcIAN",
    # "qwen3asr": "https://qwen.ai/blog?id=qwen3asr",
    # "databricks_hidden_technical_debt_genai_systems": "https://www.databricks.com/blog/hidden-technical-debt-genai-systems",
    "Qwen_blog": "https://qwen.ai/research",
    # "DeepMind_About": "https://deepmind.google/about/",
}
# ===========================================

def _clean_text_content(text):
    """
    [Optional] åå¤„ç†æ¸…æ´—æ–‡æœ¬å†…å®¹
    ç§»é™¤å¤šä½™ç©ºè¡Œã€ç‰¹å®šçš„å™ªéŸ³å…³é”®è¯è¡Œã€Cookieå£°æ˜ç­‰
    """
    if not text:
        return ""
    
    # 1. ç§»é™¤å¤šä½™ç©ºè¡Œ (ä¿ç•™æ®µè½ç»“æ„ï¼Œä½†å»é™¤å¤§ç‰‡ç©ºç™½)
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    lines = text.split('\n')
    cleaned_lines = []
    
    # å®šä¹‰ä¸€äº›ç»å¯¹ä¸æƒ³çœ‹åˆ°çš„å™ªéŸ³å…³é”®è¯ (å¤§å°å†™ä¸æ•æ„Ÿ)
    # æ³¨æ„ï¼šåªé’ˆå¯¹çŸ­è¡Œç”Ÿæ•ˆï¼Œé¿å…è¯¯åˆ æ­£æ–‡
    noise_patterns = [
        r'^share this post$',
        r'^contents in this story$',
        r'^read time:?\s*\d+',
        r'^\d+\s*min read$',
        r'^keep up with us$',
        r'^sign up for.*newsletter',
        r'^all rights reserved',
        r'^Â©\s*\d+',
        r'^click to share',
        r'^subscribe to',
        r'^share on',
    ]
    
    for line in lines:
        stripped = line.strip()
        
        # è·³è¿‡ç©ºè¡Œï¼ˆåé¢ç»Ÿä¸€ joinï¼‰
        if not stripped:
            continue
            
        # 2. æ£€æŸ¥çŸ­è¡Œå™ªéŸ³ (< 60 chars)
        if len(stripped) < 60:
            is_noise = False
            for pattern in noise_patterns:
                if re.search(pattern, stripped, re.IGNORECASE):
                    is_noise = True
                    break
            if is_noise:
                continue
        
        # 3. é’ˆå¯¹ç‰¹å®šçš„ Cookie/Privacy å£°æ˜æ®µè½ (é•¿æ–‡æœ¬ç‰¹å¾)
        lower_line = stripped.lower()
        if "cookies" in lower_line and "browser" in lower_line and "experience" in lower_line:
            continue
        if lower_line.startswith("by submitting") and "privacy policy" in lower_line:
            continue
            
        cleaned_lines.append(stripped)
        
    return '\n'.join(cleaned_lines)

def fetch_web_content(url):
    """
    [Optimized] æŠ“å–æ™®é€šç½‘é¡µå†…å®¹
    
    æ”¹è¿›ç‚¹ï¼š
    1. æ™ºèƒ½ç­‰å¾… (WebDriverWait)
    2. æ¨¡æ‹Ÿæ»šåŠ¨ (Lazy Loadæ”¯æŒ)
    3. å†…å®¹æ¸…æ´— (ç§»é™¤å¹²æ‰°æ ‡ç­¾)
    4. åçˆ¬è™«è§„é¿ä¼˜åŒ–
    """
    logger.info(f"æ­£åœ¨æŠ“å–ç½‘é¡µ(Selenium Optimized): {url} ...")
    driver = None
    try:
        # é…ç½®æ— å¤´æµè§ˆå™¨
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--window-size=1920,1080")
        
        # ä¼ªè£… User-Agent
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
        # è§„é¿æ£€æµ‹
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)

        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        # è¿›ä¸€æ­¥è§„é¿ï¼šç§»é™¤ navigator.webdriver æ ‡è®°
        driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
            "source": """
            Object.defineProperty(navigator, 'webdriver', {
              get: () => undefined
            })
            """
        })
        
        driver.get(url)
        
        # 1. æ™ºèƒ½ç­‰å¾…ï¼šç­‰å¾… body å¯è§ï¼Œæœ€é•¿ 15ç§’
        try:
            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        except Exception:
            logger.info("ç­‰å¾…é¡µé¢åŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­å¤„ç†...")

        # 2. æ¨¡æ‹Ÿæ»šåŠ¨åŠ è½½ (å¤ç”¨éƒ¨åˆ† _prepare_page_for_capture çš„ç²¾ç®€é€»è¾‘)
        # å¿«é€Ÿæ»šåŠ¨ä»¥è§¦å‘æ‡’åŠ è½½æ–‡å­—
        logger.info("-> è§¦å‘æ»šåŠ¨åŠ è½½...")
        last_height = driver.execute_script("return document.body.scrollHeight")
        for _ in range(3): # å°è¯•æ»šåŠ¨3æ¬¡ï¼Œä¸åƒæˆªå›¾é‚£æ ·éœ€è¦ç‰¹åˆ«ç²¾ç»†ï¼Œåªè¦åŠ è½½å‡ºå¤§éƒ¨åˆ†æ­£æ–‡å³å¯
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1.5)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height
        
        # è·å–æ¸²æŸ“åçš„ HTML
        html_content = driver.page_source
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # 3. å†…å®¹æ¸…æ´—
        # ç§»é™¤å¹²æ‰°å…ƒç´ 
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'noscript', 'meta', 'iframe', 'svg', 'select', 'button']):
            tag.decompose()
            
        # ç§»é™¤å¸¸è§çš„å¹¿å‘Š/ä¾§è¾¹æ  class/id
        bad_selectors = [
            '.sidebar', '#sidebar', '.ads', '.advertisement', '.social-share', 
            '.comment-list', '.related-posts', '.menu', '#menu', '.nav', '.navigation'
        ]
        for selector in bad_selectors:
            for tag in soup.select(selector):
                tag.decompose()

        # æå–æ ‡é¢˜
        title = soup.title.string.strip() if soup.title else url
        
        # 4. ä¼˜åŒ–çš„æå–ç­–ç•¥
        content_text = ""
        
        # ç­–ç•¥A: æŸ¥æ‰¾å¸¸è§çš„æ–‡ç« å®¹å™¨ ID/Class
        article_selectors = [
            'article', 
            'main',
            '[role="main"]',
            '.post-content', 
            '.entry-content', 
            '.article-content',
            '#content',
            '.container' 
        ]
        
        target_element = None
        for selector in article_selectors:
            found = soup.select(selector)
            if found:
                # å¦‚æœæ‰¾åˆ°å¤šä¸ªï¼Œå–å­—æ•°æœ€å¤šçš„ä¸€ä¸ª
                target_element = max(found, key=lambda t: len(t.get_text()))
                logger.info(f"-> å‘½ä¸­é€‰æ‹©å™¨æå–: {selector}")
                break
                
        if target_element:
            content_text = target_element.get_text(separator='\n', strip=True)
        else:
            # ç­–ç•¥B: å…œåº• - æå–æ‰€æœ‰Pæ ‡ç­¾ï¼Œä½†è¿›è¡Œå¯†åº¦è¿‡æ»¤
            logger.info("-> ä½¿ç”¨æ®µè½å¯†åº¦å›é€€ç­–ç•¥")
            paragraphs = soup.find_all('p')
            # è¿‡æ»¤æ‰è¿‡çŸ­çš„å¯¼èˆªæ€§æ–‡å­— (ä¾‹å¦‚å°‘äº 5 ä¸ªå­—)
            valid_paragraphs = [p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 5]
            content_text = "\n".join(valid_paragraphs)
            
            # ç­–ç•¥C: å¦‚æœè¿˜æ˜¯æ²¡ä¸œè¥¿ï¼ŒLast Resort
        if len(content_text) < 50 and soup.body:
                content_text = soup.body.get_text(separator='\n', strip=True)

        logger.info(f"-> åŸå§‹å†…å®¹é•¿åº¦: {len(content_text)} å­—ç¬¦")
        
        # [Optional] åå¤„ç†æ¸…æ´—
        content_text = _clean_text_content(content_text)
        logger.info(f"-> æ¸…æ´—åå†…å®¹é•¿åº¦: {len(content_text)} å­—ç¬¦")

        pub_date = datetime.now().strftime("%Y-%m-%d")
        
        return {
            "title": title,
            "date": pub_date,
            "link": url,
            "content": content_text
        }
    except Exception as e:
        logger.info(f"ç½‘é¡µæŠ“å–å¤±è´¥: {e}")
        return None
    finally:
        if driver:
            driver.quit()


def _prepare_page_for_capture(url):
    """
    å†…éƒ¨è¾…åŠ©å‡½æ•°ï¼šåˆå§‹åŒ–æµè§ˆå™¨ï¼Œæ‰“å¼€ç½‘é¡µï¼Œå¹¶æ»šåŠ¨åŠ è½½æ‰€æœ‰å†…å®¹ã€‚
    è¿”å› (driver, last_height)
    æ³¨æ„ï¼šè°ƒç”¨æ–¹è´Ÿè´£ driver.quit()
    """
    logger.info(f"æ­£åœ¨å‡†å¤‡é¡µé¢: {url} ...")
    driver = None
    try:
        # å¤ç”¨ Selenium é…ç½®
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--window-size=1920,1080") # è®¾ç½®åˆå§‹çª—å£å¤§å°
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")

        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        driver.get(url)
        
        # æ™ºèƒ½å¯»æ‰¾æ»šåŠ¨å®¹å™¨å¹¶è§¦å‘æ‡’åŠ è½½
        logger.info("-> æ­£åœ¨åˆ†æé¡µé¢ç»“æ„å¹¶åŠ è½½å†…å®¹...")
        
        # 1. æ¨¡æ‹Ÿæ»šåŠ¨ (é’ˆå¯¹æ‰¾åˆ°çš„å…ƒç´ )
        # æˆ‘ä»¬åˆ†æ®µæ»šåŠ¨ï¼Œç¡®ä¿è§¦å‘ Lazy Load
        last_height = 0
        
        # æœ€å¤šå°è¯•æ»šåŠ¨ 20 æ¬¡ï¼Œæ¯æ¬¡æ»š 1000pxï¼Œç›´åˆ°æ»šä¸åŠ¨
        for i in range(20):
            driver.execute_script("""
                let el = (function() { 
                    let maxS = 0; let target = document.documentElement;
                    [document.documentElement, document.body, ...document.querySelectorAll('div')].forEach(e => {
                        if(e.scrollHeight > maxS && e.offsetParent !== null) { maxS = e.scrollHeight; target = e; }
                    });
                    return target;
                })();
                el.scrollTop = el.scrollHeight; 
                window.scrollTo(0, document.body.scrollHeight);
            """)
            
            time.sleep(1.5) # ç­‰å¾…åŠ è½½
            
            # æ£€æŸ¥é«˜åº¦æ˜¯å¦è¿˜åœ¨å¢é•¿
            new_height = driver.execute_script("""
                let maxS = Math.max(document.body.scrollHeight, document.documentElement.scrollHeight);
                let divs = document.querySelectorAll('div');
                for(let d of divs) { if(d.scrollHeight > maxS) maxS = d.scrollHeight; }
                return maxS;
            """)
            
            if new_height == last_height and i > 2: # è‡³å°‘æ»šä¸¤æ¬¡ç¡®è®¤
                logger.info(f"-> å†…å®¹åŠ è½½å®Œæ¯•ï¼Œæ£€æµ‹åˆ°é«˜åº¦: {new_height}px")
                break
            
            last_height = new_height
            if new_height > 30000:
                logger.info("-> é¡µé¢è¿‡é•¿ï¼Œæå‰åœæ­¢")
                break
                
        # æ»šå›é¡¶éƒ¨
        driver.execute_script("window.scrollTo(0, 0)")
        return driver, last_height

    except Exception as e:
        logger.info(f"é¡µé¢å‡†å¤‡å¤±è´¥: {e}")
        if driver:
            driver.quit()
        return None, 0


def capture_web_screenshot_png(url, output_path):
    """
    æŠ“å–ç½‘é¡µé•¿æˆªå›¾ (PNG)
    """
    logger.info(f"æ­£åœ¨ç”Ÿæˆ PNG: {url} ...") 
    driver, last_height = _prepare_page_for_capture(url)
    if not driver:
        return False
        
    try:
        # æˆªå›¾: è®¾ç½®çª—å£ä¸ºæœ€å¤§æ£€æµ‹åˆ°çš„é«˜åº¦ + ç¼“å†²
        final_height = last_height + 200
        if final_height > 30000: final_height = 30000
        if final_height < 1080: final_height = 1080 # ä¿åº•
        
        logger.info(f"Final Viewport Height: {final_height}px")
        driver.set_window_size(1920, final_height)
        time.sleep(2) # å¸ƒå±€é‡ç»˜ç­‰å¾…
        driver.save_screenshot(output_path)
        logger.info(f"æ–‡ä»¶å·²ä¿å­˜è‡³: {output_path}")
        return True
    except Exception as e:
        logger.info(f"PNG ç”Ÿæˆå¤±è´¥: {e}")
        return False
    finally:
        driver.quit()


def capture_web_pdf(url, output_path):
    """
    æŠ“å–ç½‘é¡µå¹¶å¯¼å‡ºä¸ºå•é¡µé•¿ PDF
    """
    logger.info(f"æ­£åœ¨ç”Ÿæˆ PDF: {url} ...")
    driver, last_height = _prepare_page_for_capture(url)
    if not driver:
        return False

    try:
        # PDF ä¿®å¤é€»è¾‘ï¼š
        # ç›´æ¥ä½¿ç”¨åˆšæ‰æ»šåŠ¨æ¢æµ‹åˆ°çš„çœŸå®é«˜åº¦ (last_height)
        real_height = max(last_height, 1080)
        
        driver.execute_script(f"""
            // 1. å°è¯•æ‰¾åˆ°é‚£ä¸ªæ»šåŠ¨å®¹å™¨
            let scrollEl = (function() {{ 
                let maxS = 0; let target = document.body;
                [document.documentElement, document.body, ...document.querySelectorAll('div')].forEach(e => {{
                    if(e.scrollHeight > maxS && e.offsetParent !== null) {{ maxS = e.scrollHeight; target = e; }}
                }});
                return target;
            }})();
            
            // 2. æš´åŠ›æ’‘å¼€
            let h = '{real_height}px';
            
            if(scrollEl) {{
                scrollEl.style.height = h;
                scrollEl.style.maxHeight = 'none';
                scrollEl.style.overflow = 'visible';
            }}
            document.body.style.height = h;
            document.documentElement.style.height = h;
            document.body.style.overflow = 'visible';
            document.documentElement.style.overflow = 'visible';
        """)
        time.sleep(1) # ç­‰å¾…æ¸²æŸ“æ›´æ–°

        # è®¡ç®—å°ºå¯¸
        metrics = driver.execute_script("return { width: Math.max(document.body.scrollWidth, document.documentElement.scrollWidth, 1200) }")
        page_width_in_inches = metrics['width'] / 96.0
        page_height_in_inches = (real_height + 100) / 96.0 
        
        logger.info(f" -> ç”Ÿæˆ PDF å°ºå¯¸: {metrics['width']}x{real_height} px (ç”±æ»šåŠ¨æ¢æµ‹å†³å®š)")

        params = {
            'landscape': False,
            'displayHeaderFooter': False,
            'printBackground': True,
            'preferCSSPageSize': False,
            'paperWidth': page_width_in_inches,
            'paperHeight': page_height_in_inches,
            'marginTop': 0,
            'marginBottom': 0,
            'marginLeft': 0,
            'marginRight': 0,
        }
        
        result = driver.execute_cdp_cmd("Page.printToPDF", params)
        with open(output_path, 'wb') as f:
            f.write(base64.b64decode(result['data']))
        logger.info(f"æ–‡ä»¶å·²ä¿å­˜è‡³: {output_path}")
        return True
    except Exception as e:
        logger.info(f"PDF ç”Ÿæˆå¤±è´¥: {e}")
        return False
    finally:
        driver.quit()


# ================= ä¸»ç¨‹åºå…¥å£ =================
if __name__ == "__main__":
    final_report = "# ğŸŒ Web æƒ…æŠ¥å‘¨æŠ¥ (Automated)\n\n"
    
    # for name, url in web_sources.items():
    #     # ç”Ÿæˆç½‘é¡µæˆªå›¾æˆ– PDF
    #     snapshot_path = f"data/{name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
    #     capture_web_screenshot_png(url, snapshot_path)

    #     pdf_path = f"data/{name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
    #     capture_web_pdf(url, pdf_path)

    for name, url in web_sources.items():
        post = fetch_web_content(url)
        if post: # åªæœ‰æŠ“å–æˆåŠŸæ‰å¤„ç†
            logger.info(f"-> æˆåŠŸè·å–ç½‘é¡µå†…å®¹")
            final_report += f"## æ¥æºï¼š{name} (Web)\n{post}\n\n---\n\n"
    
    # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
    logger.info("\n" + "="*30 + " æœ€ç»ˆæŠ¥å‘Š " + "="*30)
    logger.info(final_report)
