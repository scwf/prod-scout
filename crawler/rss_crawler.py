"""
rss_crawler.py - RSS è®¢é˜…æŠ“å–å·¥å…·

åŠŸèƒ½ï¼š
- ä» RSSHub ç­‰æºæŠ“å–æœ€æ–°æ›´æ–°ï¼ˆå¦‚ Twitter, YouTube, åšå®¢ï¼‰
- è°ƒç”¨ LLM å¯¹æŠ“å–å†…å®¹è¿›è¡Œç»“æ„åŒ–æ•´ç†

ä¾èµ–ï¼šfeedparser, openai, python-dateutil
"""
import os
import json
import configparser
import feedparser
from datetime import datetime, timezone
from dateutil import parser as date_parser
from common import organize_data, DAYS_LOOKBACK, log

# ================= é…ç½®åŠ è½½ =================
# åŠ è½½é…ç½®æ–‡ä»¶ (config.iniï¼Œä½äºé¡¹ç›®æ ¹ç›®å½•)
config = configparser.ConfigParser()
config.optionxform = str  # ä¿ç•™ key çš„å¤§å°å†™
config.read(os.path.join(os.path.dirname(__file__), '..', 'config.ini'), encoding='utf-8')

def load_weixin_accounts_from_config():
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½å¾®ä¿¡å…¬ä¼—å·åˆ—è¡¨
    
    é…ç½®æ ¼å¼ï¼šæ˜¾ç¤ºåç§° = RSSåœ°å€
    
    è¿”å›ï¼š
        dict: {æ˜¾ç¤ºåç§°: RSSåœ°å€}
    """
    weixin_accounts = {}
    
    if config.has_section('weixin_accounts'):
        for display_name in config.options('weixin_accounts'):
            rss_url = config.get('weixin_accounts', display_name).strip()
            if rss_url:
                weixin_accounts[display_name] = rss_url
    
    return weixin_accounts

def load_x_accounts_from_config():
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½ X (Twitter) è´¦æˆ·åˆ—è¡¨
    
    é…ç½®æ ¼å¼ï¼šæ˜¾ç¤ºåç§° = è´¦æˆ·ID
    
    è¿”å›ï¼š
        dict: {æ˜¾ç¤ºåç§°: RSSåœ°å€}
    """
    x_accounts = {}
    rsshub_base_url = config.get('rsshub', 'base_url', fallback='http://127.0.0.1:1200')
    
    if config.has_section('x_accounts'):
        for display_name in config.options('x_accounts'):
            account_id = config.get('x_accounts', display_name).strip()
            if account_id:
                x_accounts[display_name] = f"{rsshub_base_url}/twitter/user/{account_id}"
    
    return x_accounts

def load_youtube_channels_from_config():
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½ YouTube é¢‘é“åˆ—è¡¨
    
    é…ç½®æ ¼å¼ï¼šæ˜¾ç¤ºåç§° = é¢‘é“ID (ä»¥UCå¼€å¤´)
    
    è¿”å›ï¼š
        dict: {æ˜¾ç¤ºåç§°: RSSåœ°å€}
    """
    youtube_channels = {}
    
    if config.has_section('youtube_channels'):
        for display_name in config.options('youtube_channels'):
            channel_id = config.get('youtube_channels', display_name).strip()
            if channel_id:
                youtube_channels[display_name] = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
    
    return youtube_channels

# ================= é…ç½®åŒºåŸŸ =================
# è®¾ç½® RSSHub çš„è®¢é˜…æº (æŒ‰æ¥æºç±»å‹åˆ†ç»„)
# æç¤ºï¼šX (Twitter) å’Œ YouTube çš„è·¯ç”±å¯ä»¥åœ¨ https://docs.rsshub.app/ æ‰¾åˆ°
rss_sources = {
    "weixin": load_weixin_accounts_from_config(),  # ä»é…ç½®æ–‡ä»¶è¯»å–å¾®ä¿¡å…¬ä¼—å·
    "X": load_x_accounts_from_config(),  # ä»é…ç½®æ–‡ä»¶è¯»å– X è´¦æˆ·
    "YouTube": load_youtube_channels_from_config(),  # ä»é…ç½®æ–‡ä»¶è¯»å– YouTube é¢‘é“
    "blog": {
        # "36Kr_News": "https://rsshub.app/36kr/newsflashes",
        # "OpenAI_Blog": "https://rsshub.app/openai/blog",
    },
}
# ===========================================


def fetch_recent_posts(rss_url, days, source_type="æœªçŸ¥", name="", save_raw=True):
    """
    æŠ“å– RSS å¹¶ç­›é€‰æŒ‡å®šå¤©æ•°å†…çš„å†…å®¹
    
    å‚æ•°ï¼š
        rss_url: RSS æºåœ°å€
        days: æŠ“å–æœ€è¿‘å¤šå°‘å¤©çš„å†…å®¹
        source_type: æ¥æºç±»å‹ï¼ˆå¾®ä¿¡å…¬ä¼—å·ã€X (Twitter)ã€YouTubeã€åšå®¢/æ–°é—»ç­‰ï¼‰
        name: æºåç§°
        save_raw: æ˜¯å¦ä¿å­˜åŸå§‹æ•°æ®ä¸º JSON å¤‡ä»½æ–‡ä»¶
    """
    log(f"æ­£åœ¨æŠ“å– [{source_type}] {name}: {rss_url} ...")
    try:
        feed = feedparser.parse(rss_url)
        
        # æ£€æŸ¥ RSS è§£ææ˜¯å¦å‡ºé”™
        if feed.bozo and not feed.entries:
            log(f"RSS è§£æå¤±è´¥: {feed.bozo_exception}")
            return []
        
        recent_posts = []
        
        # è·å–å½“å‰æ—¶é—´ (å¸¦æ—¶åŒºæ„ŸçŸ¥ï¼Œé»˜è®¤ä¸º UTC ä»¥ä¾¿æ¯”è¾ƒ)
        now = datetime.now(timezone.utc)
        
        for entry in feed.entries:
            # è§£æå‘å¸ƒæ—¶é—´
            if hasattr(entry, 'published'):
                post_date = date_parser.parse(entry.published)
            else:
                log(f"æ²¡æœ‰æ—¶é—´æˆ³: {entry}")
                continue # æ²¡æœ‰æ—¶é—´æˆ³è·³è¿‡

            # ç¡®ä¿ post_date æœ‰æ—¶åŒºä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™è®¾ä¸º UTC
            if post_date.tzinfo is None:
                post_date = post_date.replace(tzinfo=timezone.utc)
            
            # è®¡ç®—æ—¶é—´å·®
            if (now - post_date).days <= days:
                # æ¸…æ´—æ•°æ®ï¼Œæå–æ ‡é¢˜ã€é“¾æ¥å’Œæ‘˜è¦
                content = entry.get('content', '') or entry.get('description', '')
                
                recent_posts.append({
                    "title": entry.title,
                    "date": post_date.strftime("%Y-%m-%d"),
                    "link": entry.link,
                    "rss_url": rss_url,
                    "source_type": source_type,  # æ¥æºç±»å‹
                    "content": content  # ä¿ç•™åŸå§‹å†…å®¹
                })
        
        # ä¿å­˜åŸå§‹æ•°æ®ä¸º JSON å¤‡ä»½ï¼ˆç”¨äºå›æº¯å’Œé—®é¢˜å®šä½ï¼‰
        if save_raw and recent_posts:
            raw_dir = os.path.join(os.path.dirname(__file__), '..', 'data', 'raw')
            os.makedirs(raw_dir, exist_ok=True)
            # ä½¿ç”¨å®‰å…¨çš„æ–‡ä»¶åï¼šsource_type + name + æ—¶é—´æˆ³
            safe_name = "".join(c if c.isalnum() or c in ('-', '_') else '_' for c in name)
            raw_filename = f"{source_type}_{safe_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            raw_path = os.path.join(raw_dir, raw_filename)
            with open(raw_path, 'w', encoding='utf-8') as f:
                json.dump(recent_posts, f, ensure_ascii=False, indent=2)
                
        return recent_posts
    except Exception as e:
        log(f"æŠ“å–å¤±è´¥: {e}")
        return []


# ================= ä¸»ç¨‹åºå…¥å£ =================
if __name__ == "__main__":
    final_report = "# ğŸŒ Data&AI æƒ…æŠ¥å‘¨æŠ¥ (Automated RSS Crawler)\n\n"
    
    for category, sources in rss_sources.items():
        if not sources:  # è·³è¿‡ç©ºåˆ†ç±»
            continue
        
        final_report += f"## ğŸ“‚ {category}\n\n"
        
        for name, url in sources.items():
            posts = fetch_recent_posts(url, DAYS_LOOKBACK, source_type=category, name=name)
            log(f" -> å‘ç° {len(posts)} æ¡ç›¸å…³å†…å®¹ï¼Œæ­£åœ¨æ•´ç†...")
            
            organized_content = organize_data(posts, name)
            
            final_report += f"### {name}\n{organized_content}\n\n"
        
        final_report += "---\n\n"
    
    # ä¿å­˜æŠ¥å‘Šä¸º Markdown æ–‡ä»¶
    output_dir = os.path.join(os.path.dirname(__file__), '..', 'data')
    os.makedirs(output_dir, exist_ok=True)
    
    report_filename = f"rss_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    report_path = os.path.join(output_dir, report_filename)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(final_report)
    
    log(f"æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
    
    # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
    print("\n" + "="*30 + " æœ€ç»ˆæŠ¥å‘Š " + "="*30 + "\n")
    print(final_report)
