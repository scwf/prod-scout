"""
rss_crawler.py - RSS è®¢é˜…æŠ“å–å·¥å…·

åŠŸèƒ½ï¼š
- ä» RSSHub ç­‰æºæŠ“å–æœ€æ–°æ›´æ–°ï¼ˆå¦‚ Twitter, YouTube, åšå®¢ï¼‰
- è°ƒç”¨ LLM å¯¹æŠ“å–å†…å®¹è¿›è¡Œç»“æ„åŒ–æ•´ç†

ä¾èµ–ï¼šfeedparser, openai, python-dateutil
"""
import os
import json
import time
import configparser
import feedparser
from datetime import datetime, timezone
from dateutil import parser as date_parser
from common import organize_data, posts_to_markdown_table, group_posts_by_domain, DAYS_LOOKBACK, log

# ================= é…ç½®åŠ è½½ =================
# åŠ è½½é…ç½®æ–‡ä»¶ (config.iniï¼Œä½äºé¡¹ç›®æ ¹ç›®å½•)
config = configparser.ConfigParser()
config.optionxform = str  # ä¿ç•™ key çš„å¤§å°å†™
config.read(os.path.join(os.path.dirname(__file__), '..', 'config.ini'), encoding='utf-8')

def load_weixin_accounts_from_config():
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½å¾®ä¿¡å…¬ä¼—å·åˆ—è¡¨
    
    é…ç½®æ ¼å¼ï¼šæ˜¾ç¤ºåç§° = RSSåœ°å€
    
    è¿”å›ï¼š
        dict: {æ˜¾ç¤ºåç§°: RSSåœ°å€}
    """
    weixin_accounts = {}
    
    if config.has_section('weixin_accounts'):
        for display_name in config.options('weixin_accounts'):
            rss_url = config.get('weixin_accounts', display_name).strip()
            if rss_url:
                weixin_accounts[display_name] = rss_url
    
    return weixin_accounts

def load_x_accounts_from_config():
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½ X (Twitter) è´¦æˆ·åˆ—è¡¨
    
    é…ç½®æ ¼å¼ï¼šæ˜¾ç¤ºåç§° = è´¦æˆ·ID
    
    è¿”å›ï¼š
        dict: {æ˜¾ç¤ºåç§°: RSSåœ°å€}
    """
    x_accounts = {}
    rsshub_base_url = config.get('rsshub', 'base_url', fallback='http://127.0.0.1:1200')
    
    if config.has_section('x_accounts'):
        for display_name in config.options('x_accounts'):
            account_id = config.get('x_accounts', display_name).strip()
            if account_id:
                x_accounts[display_name] = f"{rsshub_base_url}/twitter/user/{account_id}"
    
    return x_accounts

def load_youtube_channels_from_config():
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½ YouTube é¢‘é“åˆ—è¡¨
    
    é…ç½®æ ¼å¼ï¼šæ˜¾ç¤ºåç§° = é¢‘é“ID (ä»¥UCå¼€å¤´)
    
    è¿”å›ï¼š
        dict: {æ˜¾ç¤ºåç§°: RSSåœ°å€}
    """
    youtube_channels = {}
    
    if config.has_section('youtube_channels'):
        for display_name in config.options('youtube_channels'):
            channel_id = config.get('youtube_channels', display_name).strip()
            if channel_id:
                youtube_channels[display_name] = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
    
    return youtube_channels

# ================= é…ç½®åŒºåŸŸ =================
# è®¾ç½® RSSHub çš„è®¢é˜…æº (æŒ‰æ¥æºç±»å‹åˆ†ç»„)
# æç¤ºï¼šX (Twitter) å’Œ YouTube çš„è·¯ç”±å¯ä»¥åœ¨ https://docs.rsshub.app/ æ‰¾åˆ°
rss_sources = {
    "weixin": load_weixin_accounts_from_config(),  # ä»é…ç½®æ–‡ä»¶è¯»å–å¾®ä¿¡å…¬ä¼—å·
    "X": load_x_accounts_from_config(),  # ä»é…ç½®æ–‡ä»¶è¯»å– X è´¦æˆ·
    "YouTube": load_youtube_channels_from_config(),  # ä»é…ç½®æ–‡ä»¶è¯»å– YouTube é¢‘é“
    "blog": {
        # "36Kr_News": "https://rsshub.app/36kr/newsflashes",
        # "OpenAI_Blog": "https://rsshub.app/openai/blog",
    },
}
# ===========================================


def fetch_recent_posts(rss_url, days, source_type="æœªçŸ¥", name="", save_raw=True):
    """
    æŠ“å– RSS å¹¶ç­›é€‰æŒ‡å®šå¤©æ•°å†…çš„å†…å®¹
    
    å‚æ•°ï¼š
        rss_url: RSS æºåœ°å€
        days: æŠ“å–æœ€è¿‘å¤šå°‘å¤©çš„å†…å®¹
        source_type: æ¥æºç±»å‹ï¼ˆå¾®ä¿¡å…¬ä¼—å·ã€X (Twitter)ã€YouTubeã€åšå®¢/æ–°é—»ç­‰ï¼‰
        name: æºåç§°
        save_raw: æ˜¯å¦ä¿å­˜åŸå§‹æ•°æ®ä¸º JSON å¤‡ä»½æ–‡ä»¶
    """
    log(f"æ­£åœ¨æŠ“å– [{source_type}] {name}: {rss_url} ...")
    try:
        feed = feedparser.parse(rss_url)
        
        # æ£€æŸ¥ RSS è§£ææ˜¯å¦å‡ºé”™
        if feed.bozo and not feed.entries:
            log(f"RSS è§£æå¤±è´¥: {feed.bozo_exception}")
            return []
        
        recent_posts = []
        
        # è·å–å½“å‰æ—¶é—´ (å¸¦æ—¶åŒºæ„ŸçŸ¥ï¼Œé»˜è®¤ä¸º UTC ä»¥ä¾¿æ¯”è¾ƒ)
        now = datetime.now(timezone.utc)
        
        for entry in feed.entries:
            # è§£æå‘å¸ƒæ—¶é—´
            if hasattr(entry, 'published'):
                post_date = date_parser.parse(entry.published)
            else:
                log(f"æ²¡æœ‰æ—¶é—´æˆ³: {entry}")
                continue # æ²¡æœ‰æ—¶é—´æˆ³è·³è¿‡

            # ç¡®ä¿ post_date æœ‰æ—¶åŒºä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™è®¾ä¸º UTC
            if post_date.tzinfo is None:
                post_date = post_date.replace(tzinfo=timezone.utc)
            
            # è®¡ç®—æ—¶é—´å·®
            if (now - post_date).days <= days:
                # æ¸…æ´—æ•°æ®ï¼Œæå–æ ‡é¢˜ã€é“¾æ¥å’Œæ‘˜è¦
                content = entry.get('content', '') or entry.get('description', '')
                
                recent_posts.append({
                    "title": entry.title,
                    "date": post_date.strftime("%Y-%m-%d"),
                    "link": entry.link,
                    "rss_url": rss_url,
                    "source_type": source_type,  # æ¥æºç±»å‹
                    "content": content  # ä¿ç•™åŸå§‹å†…å®¹
                })
        
        # ä¿å­˜åŸå§‹æ•°æ®ä¸º JSON å¤‡ä»½ï¼ˆç”¨äºå›æº¯å’Œé—®é¢˜å®šä½ï¼‰
        if save_raw and recent_posts:
            raw_dir = os.path.join(os.path.dirname(__file__), '..', 'data', 'raw')
            os.makedirs(raw_dir, exist_ok=True)
            # ä½¿ç”¨å®‰å…¨çš„æ–‡ä»¶åï¼šsource_type + name + æ—¶é—´æˆ³
            safe_name = "".join(c if c.isalnum() or c in ('-', '_') else '_' for c in name)
            raw_filename = f"{source_type}_{safe_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            raw_path = os.path.join(raw_dir, raw_filename)
            with open(raw_path, 'w', encoding='utf-8') as f:
                json.dump(recent_posts, f, ensure_ascii=False, indent=2)
                
        return recent_posts
    except Exception as e:
        log(f"æŠ“å–å¤±è´¥: {e}")
        return []


# ================= ä¸»ç¨‹åºå…¥å£ =================
if __name__ == "__main__":
    start_time = time.time()
    
    # æ”¶é›†æ‰€æœ‰æ•´ç†åçš„æ–‡ç« 
    all_organized_posts = []
    
    for category, sources in rss_sources.items():
        if not sources:  # è·³è¿‡ç©ºåˆ†ç±»
            continue
        
        log(f"ğŸ“‚ å¤„ç†åˆ†ç±»: {category}")
        
        for name, url in sources.items():
            posts = fetch_recent_posts(url, DAYS_LOOKBACK, source_type=category, name=name)
            log(f" -> å‘ç° {len(posts)} æ¡ç›¸å…³å†…å®¹ï¼Œæ­£åœ¨æ•´ç†...")
            
            # organize_data ç°åœ¨è¿”å› list[dict]
            organized_posts = organize_data(posts, name)
            all_organized_posts.extend(organized_posts)
            
            log(f" -> æ•´ç†å®Œæˆï¼Œæœ‰æ•ˆå†…å®¹ {len(organized_posts)} æ¡")
    
    # æŒ‰é¢†åŸŸåˆ†ç»„
    log(f"\nğŸ“Š å…±æ”¶é›† {len(all_organized_posts)} æ¡æœ‰æ•ˆå†…å®¹ï¼ŒæŒ‰é¢†åŸŸåˆ†ç»„...")
    grouped_posts = group_posts_by_domain(all_organized_posts)
    
    # å‡†å¤‡è¾“å‡ºç›®å½•
    output_dir = os.path.join(os.path.dirname(__file__), '..', 'data')
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    saved_files = []
    
    # ä¸ºæ¯ä¸ªé¢†åŸŸç”Ÿæˆå•ç‹¬çš„æŠ¥å‘Šæ–‡ä»¶
    for domain, posts in grouped_posts.items():
        if not posts:
            continue
        
        # ç”Ÿæˆè¯¥é¢†åŸŸçš„ Markdown æŠ¥å‘Š
        domain_report = f"# ğŸ“° Data&AI æƒ…æŠ¥å‘¨æŠ¥ - {domain}\n\n"
        domain_report += f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        domain_report += f"**å†…å®¹æ•°é‡**: {len(posts)} æ¡\n\n"
        domain_report += "---\n\n"
        
        # æŒ‰æ¥æºåˆ†ç»„æ˜¾ç¤º
        posts_by_source = {}
        for post in posts:
            source = post.get('source_name', 'æœªçŸ¥æ¥æº')
            if source not in posts_by_source:
                posts_by_source[source] = []
            posts_by_source[source].append(post)
        
        for source_name, source_posts in posts_by_source.items():
            domain_report += posts_to_markdown_table(source_posts, title=source_name)
            domain_report += "\n\n"
        
        # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶åï¼ˆæ›¿æ¢ç‰¹æ®Šå­—ç¬¦ï¼‰
        safe_domain = "".join(c if c.isalnum() or c in ('-', '_', 'ï¼ˆ', 'ï¼‰') else '_' for c in domain)
        report_filename = f"Data&AI_report_{safe_domain}_{timestamp}.md"
        report_path = os.path.join(output_dir, report_filename)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(domain_report)
        
        saved_files.append((domain, report_path, len(posts)))
        log(f"âœ… é¢†åŸŸ [{domain}] æŠ¥å‘Šå·²ä¿å­˜: {report_filename} ({len(posts)} æ¡)")
    
    # åŒæ—¶ç”Ÿæˆä¸€ä»½æ±‡æ€»æŠ¥å‘Šï¼ˆåŒ…å«æ‰€æœ‰é¢†åŸŸï¼‰
    combined_report = "# ğŸ“° Data&AI æƒ…æŠ¥å‘¨æŠ¥ (æ±‡æ€»)\n\n"
    combined_report += f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    combined_report += f"**æ€»å†…å®¹æ•°é‡**: {len(all_organized_posts)} æ¡\n\n"
    combined_report += "---\n\n"
    
    for domain, posts in grouped_posts.items():
        if not posts:
            continue
        combined_report += f"## ğŸ“‚ {domain}\n\n"
        
        # æŒ‰æ¥æºåˆ†ç»„æ˜¾ç¤º
        posts_by_source = {}
        for post in posts:
            source = post.get('source_name', 'æœªçŸ¥æ¥æº')
            if source not in posts_by_source:
                posts_by_source[source] = []
            posts_by_source[source].append(post)
        
        for source_name, source_posts in posts_by_source.items():
            combined_report += posts_to_markdown_table(source_posts, title=source_name)
            combined_report += "\n\n"
        
        combined_report += "---\n\n"
    
    combined_filename = f"Data&AI_report_æ±‡æ€»_{timestamp}.md"
    combined_path = os.path.join(output_dir, combined_filename)
    
    with open(combined_path, 'w', encoding='utf-8') as f:
        f.write(combined_report)
    
    log(f"âœ… æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {combined_filename}")
    
    # æ‰“å°æ‰§è¡Œç»“æœæ‘˜è¦
    print("\n" + "="*50)
    print("ğŸ“Š æ‰§è¡Œç»“æœæ‘˜è¦")
    print("="*50)
    print(f"æ€»å…±å¤„ç†: {len(all_organized_posts)} æ¡æœ‰æ•ˆå†…å®¹")
    print(f"é¢†åŸŸåˆ†å¸ƒ:")
    for domain, path, count in saved_files:
        print(f"  - {domain}: {count} æ¡")
    print(f"\nç”Ÿæˆæ–‡ä»¶:")
    for domain, path, count in saved_files:
        print(f"  - {os.path.basename(path)}")
    print(f"  - {combined_filename} (æ±‡æ€»)")
    
    # æ‰“å°æ—¶é—´å¼€é”€
    elapsed_time = time.time() - start_time
    print(f"\n{'='*50}")
    print(f"âœ… æ‰§è¡Œå®Œæˆï¼Œæ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
    print("="*50)
