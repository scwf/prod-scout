"""
rss_crawler.py - RSS è®¢é˜…æŠ“å–å·¥å…·

åŠŸèƒ½ï¼š
- ä» RSSHub ç­‰æºæŠ“å–æœ€æ–°æ›´æ–°ï¼ˆå¦‚ Twitter, YouTube, åšå®¢ï¼‰
- è°ƒç”¨ LLM å¯¹æŠ“å–å†…å®¹è¿›è¡Œç»“æ„åŒ–æ•´ç†

ä¾èµ–ï¼šfeedparser, openai, python-dateutil
"""
import os
import json
import time
import configparser
import feedparser
from datetime import datetime, timezone
from dateutil import parser as date_parser
from common import organize_data, posts_to_markdown_table, group_posts_by_domain, save_batch_manifest, DAYS_LOOKBACK, setup_logger

logger = setup_logger("rss_crawler")
from content_fetcher import ContentFetcher, YouTubeFetcher

# ================= é…ç½®åŠ è½½ =================
# åŠ è½½é…ç½®æ–‡ä»¶ (config.iniï¼Œä½äºé¡¹ç›®æ ¹ç›®å½•)
config = configparser.ConfigParser()
config.optionxform = str  # ä¿ç•™ key çš„å¤§å°å†™
config.read(os.path.join(os.path.dirname(__file__), '..', 'config-test.ini'), encoding='utf-8')

def load_weixin_accounts_from_config():
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½å¾®ä¿¡å…¬ä¼—å·åˆ—è¡¨
    
    é…ç½®æ ¼å¼ï¼šæ˜¾ç¤ºåç§° = RSSåœ°å€
    
    è¿”å›ï¼š
        dict: {æ˜¾ç¤ºåç§°: RSSåœ°å€}
    """
    weixin_accounts = {}
    
    if config.has_section('weixin_accounts'):
        for display_name in config.options('weixin_accounts'):
            rss_url = config.get('weixin_accounts', display_name).strip()
            if rss_url:
                weixin_accounts[display_name] = rss_url
    
    return weixin_accounts

def load_x_accounts_from_config():
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½ X (Twitter) è´¦æˆ·åˆ—è¡¨
    
    é…ç½®æ ¼å¼ï¼šæ˜¾ç¤ºåç§° = è´¦æˆ·ID
    
    è¿”å›ï¼š
        dict: {æ˜¾ç¤ºåç§°: RSSåœ°å€}
    """
    x_accounts = {}
    rsshub_base_url = config.get('rsshub', 'base_url', fallback='http://127.0.0.1:1200')
    
    if config.has_section('x_accounts'):
        for display_name in config.options('x_accounts'):
            account_id = config.get('x_accounts', display_name).strip()
            if account_id:
                x_accounts[display_name] = f"{rsshub_base_url}/twitter/user/{account_id}"
    
    return x_accounts

def load_youtube_channels_from_config():
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½ YouTube é¢‘é“åˆ—è¡¨
    
    é…ç½®æ ¼å¼ï¼šæ˜¾ç¤ºåç§° = é¢‘é“ID (ä»¥UCå¼€å¤´)
    
    è¿”å›ï¼š
        dict: {æ˜¾ç¤ºåç§°: RSSåœ°å€}
    """
    youtube_channels = {}
    
    if config.has_section('youtube_channels'):
        for display_name in config.options('youtube_channels'):
            channel_id = config.get('youtube_channels', display_name).strip()
            if channel_id:
                youtube_channels[display_name] = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
    
    return youtube_channels

# ================= é…ç½®åŒºåŸŸ =================
# è®¾ç½® RSSHub çš„è®¢é˜…æº (æŒ‰æ¥æºç±»å‹åˆ†ç»„)
# æç¤ºï¼šX (Twitter) å’Œ YouTube çš„è·¯ç”±å¯ä»¥åœ¨ https://docs.rsshub.app/ æ‰¾åˆ°
rss_sources = {
    "weixin": load_weixin_accounts_from_config(),  # ä»é…ç½®æ–‡ä»¶è¯»å–å¾®ä¿¡å…¬ä¼—å·
    "X": load_x_accounts_from_config(),  # ä»é…ç½®æ–‡ä»¶è¯»å– X è´¦æˆ·
    "YouTube": load_youtube_channels_from_config(),  # ä»é…ç½®æ–‡ä»¶è¯»å– YouTube é¢‘é“
    "blog": {
        # "36Kr_News": "https://rsshub.app/36kr/newsflashes",
        # "OpenAI_Blog": "https://rsshub.app/openai/blog",
    },
}

# ================= å†…å®¹å¢å¼ºæ¨¡å— =================
# ç”¨äºä»Xæ¨æ–‡ä¸­æå–åµŒå…¥é“¾æ¥å†…å®¹ï¼Œä»¥åŠä»YouTubeè§†é¢‘ä¸­æå–å­—å¹•
content_fetcher = ContentFetcher()
youtube_fetcher = YouTubeFetcher()
# ===========================================


# ================= è¾…åŠ©å‡½æ•° =================

def _parse_date(entry):
    """è§£æå¹¶æ ‡å‡†åŒ–æ—¶é—´"""
    if not hasattr(entry, 'published'): return None
    dt = date_parser.parse(entry.published)
    return dt.replace(tzinfo=timezone.utc) if dt.tzinfo is None else dt

def _enrich_x_content(content, title):
    """æå– X æ¨æ–‡çš„åµŒå…¥å†…å®¹"""
    try:
        embedded, extra_urls = content_fetcher.fetch_embedded_content(content)
        extra_content = ""
        if embedded:
            parts = [f"[{'åšå®¢' if i.content_type == 'blog' else 'è§†é¢‘å­—å¹•'}] {i.content}" 
                     for i in embedded if i.content]
            extra_content = "\n\n".join(parts)
        
        if embedded or extra_urls:
            t = (title or "æ— æ ‡é¢˜")
            t = t[:30] + "..." if len(t) > 30 else t
            logger.info(f"[{t}] åµŒå…¥: {len(embedded)}, å¤–é“¾: {len(extra_urls)}")
        return extra_content, extra_urls
    except Exception as e:
        logger.info(f"Xå†…å®¹æå–å¤±è´¥: {e}")
        return "", []

def _enrich_youtube_content(link, title, context=""):
    """æå– YouTube å­—å¹•
    
    å‚æ•°:
        link: è§†é¢‘é“¾æ¥
        title: è§†é¢‘æ ‡é¢˜
        context: ä¸Šä¸‹æ–‡ï¼ˆé€šå¸¸æ˜¯RSSæ‘˜è¦/æè¿°ï¼‰
    """
    try:
        # ä¼ é€’ title å’Œ context åˆ° fetchï¼Œcontext ç”¨ä½œè¡¥å……ä¿¡æ¯
        full_context = f"{title}\n{context}" if context else title
        yt = youtube_fetcher.fetch(link, context=full_context)
        if yt and yt.content:
            logger.info(f"æå–åˆ°å­—å¹•: {len(yt.content)} å­—ç¬¦")
            return yt.content
    except Exception as e:
        logger.info(f"å­—å¹•æå–å¤±è´¥: {e}")
    return ""

def _save_raw_backup(posts, source_type, name):
    """ä¿å­˜åŸå§‹æ•°æ®å¤‡ä»½"""
    if not posts: return
    try:
        raw_dir = os.path.join(os.path.dirname(__file__), '..', 'data', 'raw')
        os.makedirs(raw_dir, exist_ok=True)
        safe_name = "".join(c if c.isalnum() or c in '-_' else '_' for c in name)
        filename = f"{source_type}_{safe_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(os.path.join(raw_dir, filename), 'w', encoding='utf-8') as f:
            json.dump(posts, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.info(f"å¤‡ä»½å¤±è´¥: {e}")


def fetch_recent_posts(rss_url, days, source_type="æœªçŸ¥", name="", save_raw=True):
    """
    æŠ“å– RSS å¹¶ç­›é€‰æŒ‡å®šå¤©æ•°å†…çš„å†…å®¹
    
    å‚æ•°ï¼š
        rss_url: RSS æºåœ°å€
        days: æŠ“å–æœ€è¿‘å¤šå°‘å¤©çš„å†…å®¹
        source_type: æ¥æºç±»å‹ï¼ˆå¾®ä¿¡å…¬ä¼—å·ã€X (Twitter)ã€YouTubeã€åšå®¢/æ–°é—»ç­‰ï¼‰
        name: æºåç§°
        save_raw: æ˜¯å¦ä¿å­˜åŸå§‹æ•°æ®ä¸º JSON å¤‡ä»½æ–‡ä»¶
    """
    logger.info(f"æ­£åœ¨æŠ“å– [{source_type}] {name}: {rss_url} ...")
    try:
        feed = feedparser.parse(rss_url)
        
        # æ£€æŸ¥ RSS è§£ææ˜¯å¦å‡ºé”™
        if feed.bozo and not feed.entries:
            logger.info(f"RSS è§£æå¤±è´¥: {feed.bozo_exception}")
            return []
        
        recent_posts = []
        
        # è·å–å½“å‰æ—¶é—´ (å¸¦æ—¶åŒºæ„ŸçŸ¥ï¼Œé»˜è®¤ä¸º UTC ä»¥ä¾¿æ¯”è¾ƒ)
        now = datetime.now(timezone.utc)
        
        for entry in feed.entries:
            # 1. æ—¶é—´æ£€æŸ¥
            post_date = _parse_date(entry)
            if not post_date or (now - post_date).days > days:
                continue

            # 2. åŸºç¡€å†…å®¹æå–
            content = entry.get('content', '') or entry.get('description', '')
            extra_content, extra_urls = '', []

            logger.info(f"æ ‡é¢˜: {entry.title}")

            # 3. å†…å®¹å¢å¼º (X/YouTube)
            if source_type == "X":
                extra_content, extra_urls = _enrich_x_content(content, entry.title)
            elif source_type == "YouTube":
                extra_content = _enrich_youtube_content(entry.link, entry.title, content)

            recent_posts.append({
                "title": entry.title,
                "date": post_date.strftime("%Y-%m-%d"),
                "link": entry.link,
                "rss_url": rss_url,
                "source_type": source_type,
                "content": content,
                "extra_content": extra_content,
                "extra_urls": extra_urls
            })
        
        # ä¿å­˜å¤‡ä»½
        if save_raw:
            _save_raw_backup(recent_posts, source_type, name)
                
        return recent_posts
    except Exception as e:
        logger.info(f"æŠ“å–å¤±è´¥: {e}")
        return []


# ================= ä¸»ç¨‹åºå…¥å£ =================
if __name__ == "__main__":
    start_time = time.time()
    
    # æ”¶é›†æ‰€æœ‰æ•´ç†åçš„æ–‡ç« 
    all_organized_posts = []
    
    for category, sources in rss_sources.items():
        if not sources:  # è·³è¿‡ç©ºåˆ†ç±»
            continue
        
        logger.info(f"ğŸ“‚ å¤„ç†åˆ†ç±»: {category}")
        
        for name, url in sources.items():
            posts = fetch_recent_posts(url, DAYS_LOOKBACK, source_type=category, name=name)
            logger.info(f"-> å‘ç° {len(posts)} æ¡ç›¸å…³å†…å®¹ï¼Œä½¿ç”¨LLMè¿›è¡Œæ•´ç†...")
            
            # organize_data ç°åœ¨è¿”å› list[dict]
            organized_posts = organize_data(posts, name)
            all_organized_posts.extend(organized_posts)
            
            logger.info(f"-> æ•´ç†å®Œæˆï¼Œæœ‰æ•ˆå†…å®¹ {len(organized_posts)} æ¡")
    
    # æŒ‰é¢†åŸŸåˆ†ç»„
    logger.info(f"\nğŸ“Š æ•´ç†å®Œï¼Œå…± {len(all_organized_posts)} æ¡æœ‰æ•ˆå†…å®¹ï¼ŒæŒ‰é¢†åŸŸåˆ†ç»„...")
    grouped_posts = group_posts_by_domain(all_organized_posts)
    
    # å‡†å¤‡è¾“å‡ºç›®å½•
    output_dir = os.path.join(os.path.dirname(__file__), '..', 'data')
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    saved_files = []
    domain_report_files = {}  # ç”¨äºæ¸…å•: {é¢†åŸŸåç§°: æ–‡ä»¶å}
    
    # ä¸ºæ¯ä¸ªé¢†åŸŸç”Ÿæˆå•ç‹¬çš„æŠ¥å‘Šæ–‡ä»¶
    for domain, posts in grouped_posts.items():
        if not posts:
            continue
        
        # ç”Ÿæˆè¯¥é¢†åŸŸçš„ Markdown æŠ¥å‘Š
        domain_report = f"# ğŸ“° Data&AI æƒ…æŠ¥å‘¨æŠ¥ - {domain}\n\n"
        domain_report += f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        domain_report += f"**å†…å®¹æ•°é‡**: {len(posts)} æ¡\n\n"
        domain_report += "---\n\n"
        
        # æŒ‰æ¥æºåˆ†ç»„æ˜¾ç¤º
        posts_by_source = {}
        for post in posts:
            source = post.get('source_name', 'æœªçŸ¥æ¥æº')
            if source not in posts_by_source:
                posts_by_source[source] = []
            posts_by_source[source].append(post)
        
        for source_name, source_posts in posts_by_source.items():
            domain_report += posts_to_markdown_table(source_posts, title=source_name)
            domain_report += "\n\n"
        
        # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶åï¼ˆæ›¿æ¢ç‰¹æ®Šå­—ç¬¦ï¼‰
        safe_domain = "".join(c if c.isalnum() or c in ('-', '_', 'ï¼ˆ', 'ï¼‰') else '_' for c in domain)
        report_filename = f"Data&AI_report_{safe_domain}_{timestamp}.md"
        report_path = os.path.join(output_dir, report_filename)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(domain_report)
        
        saved_files.append((domain, report_path, len(posts)))
        domain_report_files[domain] = report_filename  # è®°å½•åˆ°æ¸…å•
        logger.info(f"âœ… é¢†åŸŸ [{domain}] æŠ¥å‘Šå·²ä¿å­˜: {report_filename} ({len(posts)} æ¡)")
    
    # ä¿å­˜æ‰¹æ¬¡æ¸…å•æ–‡ä»¶
    save_batch_manifest(
        output_dir=output_dir,
        batch_id=timestamp,
        domain_reports=domain_report_files,
        stats={
            "total_posts": len(all_organized_posts),
            "domain_count": len(domain_report_files)
        }
    )
    
    # æ‰“å°æ‰§è¡Œç»“æœæ‘˜è¦
    print("\n" + "="*50)
    print("ğŸ“Š æ‰§è¡Œç»“æœæ‘˜è¦")
    print("="*50)
    print(f"æ€»å…±å¤„ç†: {len(all_organized_posts)} æ¡æœ‰æ•ˆå†…å®¹")
    print(f"é¢†åŸŸåˆ†å¸ƒ:")
    for domain, path, count in saved_files:
        print(f"  - {domain}: {count} æ¡")
    print(f"\nç”Ÿæˆæ–‡ä»¶:")
    for domain, path, count in saved_files:
        print(f"  - {os.path.basename(path)}")
    
    # æ‰“å°æ—¶é—´å¼€é”€
    elapsed_time = time.time() - start_time
    print(f"\n{'='*50}")
    print(f"âœ… æ‰§è¡Œå®Œæˆï¼Œæ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
    print("="*50)
