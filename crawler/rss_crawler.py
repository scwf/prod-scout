"""
rss_crawler.py - RSS è®¢é˜…æŠ“å–å·¥å…·

åŠŸèƒ½ï¼š
- ä» RSSHub ç­‰æºæŠ“å–æœ€æ–°æ›´æ–°ï¼ˆå¦‚ Twitter, YouTube, åšå®¢ï¼‰
- è°ƒç”¨ LLM å¯¹æŠ“å–å†…å®¹è¿›è¡Œç»“æ„åŒ–æ•´ç†

ä¾èµ–ï¼šfeedparser, openai, python-dateutil
"""
import os
import json
import time
import configparser
import feedparser
from datetime import datetime, timezone
from dateutil import parser as date_parser
from concurrent.futures import ThreadPoolExecutor, as_completed
from common import organize_single_post, group_posts_by_domain, save_batch_manifest, DAYS_LOOKBACK, setup_logger

logger = setup_logger("rss_crawler")
from content_fetcher import ContentFetcher

# ================= é…ç½®åŠ è½½ =================
# åŠ è½½é…ç½®æ–‡ä»¶ (config.iniï¼Œä½äºé¡¹ç›®æ ¹ç›®å½•)
config = configparser.ConfigParser()
config.optionxform = str  # ä¿ç•™ key çš„å¤§å°å†™
config.read(os.path.join(os.path.dirname(__file__), '..', 'config-test.ini'), encoding='utf-8')

def load_weixin_accounts_from_config():
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½å¾®ä¿¡å…¬ä¼—å·åˆ—è¡¨
    
    é…ç½®æ ¼å¼ï¼šæ˜¾ç¤ºåç§° = RSSåœ°å€
    
    è¿”å›ï¼š
        dict: {æ˜¾ç¤ºåç§°: RSSåœ°å€}
    """
    weixin_accounts = {}
    
    if config.has_section('weixin_accounts'):
        for display_name in config.options('weixin_accounts'):
            rss_url = config.get('weixin_accounts', display_name).strip()
            if rss_url:
                weixin_accounts[display_name] = rss_url
    
    return weixin_accounts

def load_x_accounts_from_config():
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½ X (Twitter) è´¦æˆ·åˆ—è¡¨
    
    é…ç½®æ ¼å¼ï¼šæ˜¾ç¤ºåç§° = è´¦æˆ·ID
    
    è¿”å›ï¼š
        dict: {æ˜¾ç¤ºåç§°: RSSåœ°å€}
    """
    x_accounts = {}
    rsshub_base_url = config.get('rsshub', 'base_url', fallback='http://127.0.0.1:1200')
    
    if config.has_section('x_accounts'):
        for display_name in config.options('x_accounts'):
            account_id = config.get('x_accounts', display_name).strip()
            if account_id:
                x_accounts[display_name] = f"{rsshub_base_url}/twitter/user/{account_id}"
    
    return x_accounts

def load_youtube_channels_from_config():
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½ YouTube é¢‘é“åˆ—è¡¨
    
    é…ç½®æ ¼å¼ï¼šæ˜¾ç¤ºåç§° = é¢‘é“ID (ä»¥UCå¼€å¤´)
    
    è¿”å›ï¼š
        dict: {æ˜¾ç¤ºåç§°: RSSåœ°å€}
    """
    youtube_channels = {}
    
    if config.has_section('youtube_channels'):
        for display_name in config.options('youtube_channels'):
            channel_id = config.get('youtube_channels', display_name).strip()
            if channel_id:
                youtube_channels[display_name] = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
    
    return youtube_channels

# ================= é…ç½®åŒºåŸŸ =================
# è®¾ç½® RSSHub çš„è®¢é˜…æº (æŒ‰æ¥æºç±»å‹åˆ†ç»„)
# æç¤ºï¼šX (Twitter) å’Œ YouTube çš„è·¯ç”±å¯ä»¥åœ¨ https://docs.rsshub.app/ æ‰¾åˆ°
rss_sources = {
    "weixin": load_weixin_accounts_from_config(),  # ä»é…ç½®æ–‡ä»¶è¯»å–å¾®ä¿¡å…¬ä¼—å·
    "X": load_x_accounts_from_config(),  # ä»é…ç½®æ–‡ä»¶è¯»å– X è´¦æˆ·
    "YouTube": load_youtube_channels_from_config(),  # ä»é…ç½®æ–‡ä»¶è¯»å– YouTube é¢‘é“
}

# ================= å†…å®¹å¢å¼ºæ¨¡å— =================
# ç”¨äºä»Xæ¨æ–‡ä¸­æå–åµŒå…¥é“¾æ¥å†…å®¹ï¼Œä»¥åŠä»YouTubeè§†é¢‘ä¸­æå–å­—å¹•
content_fetcher = ContentFetcher()
# ===========================================


def generate_post_markdown(post, domain):
    """ç”Ÿæˆå•ç¯‡æ–‡ç« çš„ Markdown å†…å®¹"""
    lines = [
        f"# {post.get('event', 'æœªå‘½åäº‹ä»¶')}",
        "",
        f"- **æ—¥æœŸ**: {post.get('date', 'æœªçŸ¥æ—¥æœŸ')}",
        f"- **äº‹ä»¶åˆ†ç±»**: {post.get('category', 'æœªåˆ†ç±»')}",
        f"- **æ‰€å±é¢†åŸŸ**: {domain}",
        f"- **æ˜¯å¦å±äºæ´å¯ŸèŒƒå›´**: {'âœ… æ˜¯' if post.get('is_in_scope') else 'âŒ å¦'}",
        f"- **åˆ¤æ–­ç†ç”±**: {post.get('scope_reason', 'æ— ')}",
        f"- **æ¥æº**: {post.get('source_name', 'æœªçŸ¥')}",
        f"- **åŸæ–‡é“¾æ¥**: {post.get('link', '')}",
        "",
        "## å…³é”®ä¿¡æ¯",
        post.get('key_info', ''),
        "",
        "## è¯¦ç»†å†…å®¹",
        post.get('detail', ''),
        "",
    ]
    
    if post.get('extra_content'):
        lines.extend(["â€‹## è¡¥å……å†…å®¹", post['extra_content'], ""])
    
    if post.get('extra_urls'):
        lines.append("## å¤–éƒ¨é“¾æ¥")
        lines.extend([f"- {url}" for url in post['extra_urls']])
        lines.append("")
    
    return "\n".join(lines)


# ================= è¾…åŠ©å‡½æ•° =================

def _parse_date(entry):
    """è§£æå¹¶æ ‡å‡†åŒ–æ—¶é—´"""
    if not hasattr(entry, 'published'): return None
    dt = date_parser.parse(entry.published)
    return dt.replace(tzinfo=timezone.utc) if dt.tzinfo is None else dt

def _enrich_x_content(content, title):
    """æå– X æ¨æ–‡çš„åµŒå…¥å†…å®¹"""
    try:
        enable_opt = config.getboolean('llm', 'enable_subtitle_optimization', fallback=False)
        embedded, extra_urls = content_fetcher.fetch_embedded_content(content, title=title, optimize_video=enable_opt)
        extra_content = ""
        if embedded:
            parts = [f"[{'åšå®¢' if i.content_type == 'blog' else 'è§†é¢‘å­—å¹•'}] {i.content}" 
                     for i in embedded if i.content]
            extra_content = "\n\n".join(parts)
        
        if embedded or extra_urls:
            t = (title or "æ— æ ‡é¢˜")
            t = t[:30] + "..." if len(t) > 30 else t
            logger.info(f"[{t}] åµŒå…¥: {len(embedded)}, å¤–é“¾: {len(extra_urls)}")
        return extra_content, extra_urls
    except Exception as e:
        logger.info(f"Xå†…å®¹æå–å¤±è´¥: {e}")
        return "", []

def _enrich_youtube_content(link, title, context=""):
    """æå– YouTube å­—å¹•
    
    å‚æ•°:
        link: è§†é¢‘é“¾æ¥
        title: è§†é¢‘æ ‡é¢˜
        context: ä¸Šä¸‹æ–‡ï¼ˆé€šå¸¸æ˜¯RSSæ‘˜è¦/æè¿°ï¼‰
    """
    try:
        # ä¼ é€’ title å’Œ context åˆ° fetchï¼Œcontext ç”¨ä½œè¡¥å……ä¿¡æ¯
        full_context = f"{title}\n{context}" if context else title
        # ä»é…ç½®è¯»å–æ˜¯å¦å¯ç”¨å­—å¹•ä¼˜åŒ–
        enable_opt = config.getboolean('llm', 'enable_subtitle_optimization', fallback=False)
        yt = content_fetcher.video_fetcher.fetch(link, context=full_context, title=title, optimize=enable_opt)
        if yt and yt.content:
            logger.info(f"æå–åˆ°å­—å¹•: {len(yt.content)} å­—ç¬¦")
            return yt.content
    except Exception as e:
        logger.info(f"å­—å¹•æå–å¤±è´¥: {e}")
    return ""

def _save_raw_backup(posts, source_type, name):
    """ä¿å­˜åŸå§‹æ•°æ®å¤‡ä»½"""
    if not posts: return
    try:
        raw_dir = os.path.join(os.path.dirname(__file__), '..', 'data', 'raw')
        os.makedirs(raw_dir, exist_ok=True)
        safe_name = "".join(c if c.isalnum() or c in '-_' else '_' for c in name)
        filename = f"{source_type}_{safe_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(os.path.join(raw_dir, filename), 'w', encoding='utf-8') as f:
            json.dump(posts, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.info(f"å¤‡ä»½å¤±è´¥: {e}")


def fetch_recent_posts(rss_url, days, source_type="æœªçŸ¥", name="", save_raw=True):
    """
    æŠ“å– RSS å¹¶ç­›é€‰æŒ‡å®šå¤©æ•°å†…çš„å†…å®¹
    
    å‚æ•°ï¼š
        rss_url: RSS æºåœ°å€
        days: æŠ“å–æœ€è¿‘å¤šå°‘å¤©çš„å†…å®¹
        source_type: æ¥æºç±»å‹ï¼ˆå¾®ä¿¡å…¬ä¼—å·ã€X (Twitter)ã€YouTubeã€åšå®¢/æ–°é—»ç­‰ï¼‰
        name: æºåç§°
        save_raw: æ˜¯å¦ä¿å­˜åŸå§‹æ•°æ®ä¸º JSON å¤‡ä»½æ–‡ä»¶
    """
    logger.info(f"æ­£åœ¨æŠ“å– [{source_type}] {name}: {rss_url} ...")
    try:
        feed = feedparser.parse(rss_url)
        
        # æ£€æŸ¥ RSS è§£ææ˜¯å¦å‡ºé”™
        if feed.bozo and not feed.entries:
            logger.info(f"RSS è§£æå¤±è´¥: {feed.bozo_exception}")
            return []
        
        recent_posts = []
        
        # è·å–å½“å‰æ—¶é—´ (å¸¦æ—¶åŒºæ„ŸçŸ¥ï¼Œé»˜è®¤ä¸º UTC ä»¥ä¾¿æ¯”è¾ƒ)
        now = datetime.now(timezone.utc)
        
        for entry in feed.entries:
            # 1. æ—¶é—´æ£€æŸ¥
            post_date = _parse_date(entry)
            if not post_date or (now - post_date).days > days:
                continue

            # 2. åŸºç¡€å†…å®¹æå–
            content = entry.get('content', '') or entry.get('description', '')
            extra_content, extra_urls = '', []

            logger.info(f"æ ‡é¢˜: {entry.title}")

            # 3. å†…å®¹å¢å¼º (X/YouTube)
            if source_type == "X":
                extra_content, extra_urls = _enrich_x_content(content, entry.title)
            elif source_type == "YouTube":
                extra_content = _enrich_youtube_content(entry.link, entry.title, content)

            recent_posts.append({
                "title": entry.title,
                "date": post_date.strftime("%Y-%m-%d"),
                "link": entry.link,
                "rss_url": rss_url,
                "source_type": source_type,
                "content": content,
                "extra_content": extra_content,
                "extra_urls": extra_urls
            })
        
        # ä¿å­˜å¤‡ä»½
        if save_raw:
            _save_raw_backup(recent_posts, source_type, name)
                
        return recent_posts
    except Exception as e:
        logger.info(f"æŠ“å–å¤±è´¥: {e}")
        return []


# ================= ä¸»ç¨‹åºå…¥å£ =================
if __name__ == "__main__":
    start_time = time.time()
    MAX_WORKERS = config.getint('crawler', 'organize_workers', fallback=5)
    
    # å‡†å¤‡è¾“å‡ºç›®å½•
    output_dir = os.path.join(os.path.dirname(__file__), '..', 'data')
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    os.makedirs(output_dir, exist_ok=True)
    
    # ç”¨äºè¿½è¸ªå·²åˆ›å»ºçš„é¢†åŸŸç›®å½• {domain: (dir_path, file_count)}
    domain_dirs = {}
    
    def get_domain_dir(domain):
        """è·å–é¢†åŸŸç›®å½•è·¯å¾„ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º"""
        if domain not in domain_dirs:
            safe_domain = "".join(c if c.isalnum() or c in ('-', '_') else '_' for c in domain)
            dir_name = f"{safe_domain}_{timestamp}"
            dir_path = os.path.join(output_dir, dir_name)
            os.makedirs(dir_path, exist_ok=True)
            domain_dirs[domain] = {'path': dir_path, 'name': dir_name, 'count': 0}
        return domain_dirs[domain]
    
    def write_post_file(result):
        """å°†å•ç¯‡æ–‡ç« å†™å…¥å¯¹åº”é¢†åŸŸç›®å½•"""
        domain = result.get('domain', 'å…¶ä»–')
        event = result.get('event', 'æœªå‘½åäº‹ä»¶')
        date_str = result.get('date', 'æœªçŸ¥æ—¥æœŸ')
        
        domain_info = get_domain_dir(domain)
        
        safe_event = "".join(c if c.isalnum() or c in ('-', '_') else '_' for c in event)[:50]
        filename = f"{safe_event}_{date_str}.md"
        filepath = os.path.join(domain_info['path'], filename)
        
        md_content = generate_post_markdown(result, domain)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        domain_info['count'] += 1
    
    # 1. å‡†å¤‡æºåˆ—è¡¨
    sources_list = [
        (category, name, url) 
        for category, sources in rss_sources.items()
        for name, url in sources.items()
    ]
    
    logger.info(f"ğŸš€ å¼€å§‹å¤„ç† {len(sources_list)} ä¸ªè®¢é˜…æº (é¡ºåºæŠ“å– -> å¹¶è¡Œæ•´ç†)...")
    
    all_organized_posts = []
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # 2. ä¸²è¡ŒæŠ“å–æ‰€æœ‰æº
        all_posts = []
        for category, name, url in sources_list:
            posts = fetch_recent_posts(url, DAYS_LOOKBACK, source_type=category, name=name)
            if posts:
                logger.info(f"-> [{name}] è·å– {len(posts)} æ¡")
                all_posts.extend((post, name) for post in posts)
        
        logger.info(f"å…±è·å– {len(all_posts)} ç¯‡æ–‡ç« ï¼Œæäº¤å¹¶è¡Œæ•´ç†...")
        
        # 3. å¹¶è¡Œæ•´ç†ï¼ˆæ¯ç¯‡æ–‡ç« ä¸€ä¸ªä»»åŠ¡ï¼‰
        futures = {
            executor.submit(organize_single_post, post, name): (post, name)
            for post, name in all_posts
        }
        
        # 4. è·å–ç»“æœ & å³æ—¶å†™å…¥
        completed = 0
        for future in as_completed(futures):
            post, name = futures[future]
            completed += 1
            try:
                result = future.result()
                if result:
                    all_organized_posts.append(result)
                    write_post_file(result)  # å³æ—¶å†™å…¥
            except Exception as e:
                logger.error(f"âŒ [{name}] æ•´ç†å¤±è´¥: {e}")
            
            if completed % 10 == 0:
                logger.info(f"è¿›åº¦: {completed}/{len(futures)}")
    
    logger.info(f"æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼Œå…±è·å– {len(all_organized_posts)} æ¡æœ‰æ•ˆå†…å®¹")
    
    # 5. ä¿å­˜æ‰¹æ¬¡æ¸…å•
    domain_report_dirs = {domain: info['name'] for domain, info in domain_dirs.items()}
    save_batch_manifest(
        output_dir=output_dir,
        batch_id=timestamp,
        domain_reports=domain_report_dirs,
        stats={
            "total_posts": len(all_organized_posts),
            "domain_count": len(domain_dirs)
        }
    )
    
    # æ‰“å°æ‰§è¡Œç»“æœæ‘˜è¦
    print("\n" + "="*50)
    print("ğŸ“Š æ‰§è¡Œç»“æœæ‘˜è¦")
    print("="*50)
    print(f"æ€»å…±å¤„ç†: {len(all_organized_posts)} æ¡æœ‰æ•ˆå†…å®¹")
    print(f"é¢†åŸŸåˆ†å¸ƒ:")
    for domain, info in domain_dirs.items():
        print(f"  - {domain}: {info['count']} æ¡")
        logger.info(f"âœ… é¢†åŸŸ [{domain}] å·²ä¿å­˜ {info['count']} ä¸ªæ–‡ä»¶")
    print(f"\nç”Ÿæˆç›®å½•:")
    for domain, info in domain_dirs.items():
        print(f"  - {info['name']}")
    
    elapsed_time = time.time() - start_time
    print(f"\n{'='*50}")
    print(f"âœ… æ‰§è¡Œå®Œæˆï¼Œæ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
    print("="*50)
