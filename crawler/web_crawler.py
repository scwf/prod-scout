"""
web_crawler.py - Web é¡µé¢æŠ“å–å·¥å…·

åŠŸèƒ½ï¼š
- ä½¿ç”¨ Selenium æŠ“å–æ™®é€šç½‘é¡µå†…å®¹
- ç”Ÿæˆç½‘é¡µé•¿æˆªå›¾ (PNG) å’Œé«˜ä¿çœŸ PDF å­˜æ¡£
- è°ƒç”¨ LLM å¯¹æŠ“å–å†…å®¹è¿›è¡Œç»“æ„åŒ–æ•´ç†

ä¾èµ–ï¼šselenium, beautifulsoup4, openai, webdriver-manager
"""
import base64
from datetime import datetime
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import time
from common import organize_data, log

# ================= é…ç½®åŒºåŸŸ =================
# è®¾ç½®æ™®é€š Web URL æŠ“å–æº
# é€‚ç”¨äºæ²¡æœ‰ RSS çš„å•é¡µé¢ï¼Œå¦‚å…·ä½“çš„ä¸€ç¯‡åšæ–‡æˆ–é™æ€é¡µé¢
web_sources = {
    # "Qwen_blog": "https://qwen.ai/research",
    # "DeepMind_About": "https://deepmind.google/about/",
}
# ===========================================


def fetch_web_content(url):
    """
    æŠ“å–æ™®é€šç½‘é¡µå†…å®¹ (ä½¿ç”¨ Selenium ä»¥æ”¯æŒåŠ¨æ€æ¸²æŸ“)
    """
    log(f"    æ­£åœ¨æŠ“å–ç½‘é¡µ(Selenium): {url} ...")
    driver = None
    try:
        # é…ç½®æ— å¤´æµè§ˆå™¨
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # æ— ç•Œé¢æ¨¡å¼
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        # ä¼ªè£… User-Agent
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")

        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        driver.get(url)
        # ç­‰å¾…é¡µé¢åŠ è½½ (ç®€å•ç­‰å¾…ï¼Œå¯æ”¹è¿›ä¸º WebDriverWait)
        time.sleep(5) 
        
        # è·å–æ¸²æŸ“åçš„ HTML
        html_content = driver.page_source
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = soup.title.string.strip() if soup.title else url
        
        # æå–æ­£æ–‡ 
        # ç­–ç•¥ï¼šä¼˜å…ˆæ‰¾ article æ ‡ç­¾ï¼Œå…¶æ¬¡æ‰¾ä¸»è¦çš„ div ç±»åï¼Œæœ€åå…œåº• p æ ‡ç­¾
        content_text = ""
        
        # å°è¯•é€šå¸¸çš„æ–‡ç« å®¹å™¨
        article = soup.find('article')
        if article:
            content_text = article.get_text(strip=True)
        else:
            # å…œåº•ï¼šè·å–æ‰€æœ‰ p æ ‡ç­¾
            paragraphs = soup.find_all('p')
            content_text = "\n".join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
            
            # æœ€åçš„å…œåº•ï¼šbody
            if not content_text and soup.body:
                content_text = soup.body.get_text(strip=True)

        # DEBUG: æ‰“å°æŠ“å–åˆ°çš„å†…å®¹æ—¥å¿—
        # log(f"[DEBUG] Title: {title}")
        # log(f"[DEBUG] Content Length: {len(content_text)}")
        # preview = content_text[:200].replace('\n', ' ')
        # log(f"[DEBUG] Content Preview (first 200 chars): {preview}...")

        # æ™®é€šç½‘é¡µé€šå¸¸æ²¡æœ‰ç»Ÿä¸€çš„ "å‘å¸ƒæ—¶é—´" å…ƒæ•°æ®ï¼Œè¿™é‡Œä½¿ç”¨å½“å‰æŠ“å–æ—¶é—´ä½œä¸ºå‚è€ƒ
        pub_date = datetime.now().strftime("%Y-%m-%d")
        
        return {
            "title": title,
            "date": pub_date,
            "link": url,
            "content": content_text
        }
    except Exception as e:
        log(f"    ç½‘é¡µæŠ“å–å¤±è´¥: {e}")
        return None
    finally:
        if driver:
            driver.quit()


def _prepare_page_for_capture(url):
    """
    å†…éƒ¨è¾…åŠ©å‡½æ•°ï¼šåˆå§‹åŒ–æµè§ˆå™¨ï¼Œæ‰“å¼€ç½‘é¡µï¼Œå¹¶æ»šåŠ¨åŠ è½½æ‰€æœ‰å†…å®¹ã€‚
    è¿”å› (driver, last_height)
    æ³¨æ„ï¼šè°ƒç”¨æ–¹è´Ÿè´£ driver.quit()
    """
    log(f"    æ­£åœ¨å‡†å¤‡é¡µé¢: {url} ...")
    driver = None
    try:
        # å¤ç”¨ Selenium é…ç½®
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--window-size=1920,1080") # è®¾ç½®åˆå§‹çª—å£å¤§å°
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")

        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        driver.get(url)
        
        # æ™ºèƒ½å¯»æ‰¾æ»šåŠ¨å®¹å™¨å¹¶è§¦å‘æ‡’åŠ è½½
        log("    -> æ­£åœ¨åˆ†æé¡µé¢ç»“æ„å¹¶åŠ è½½å†…å®¹...")
        
        # 1. æ¨¡æ‹Ÿæ»šåŠ¨ (é’ˆå¯¹æ‰¾åˆ°çš„å…ƒç´ )
        # æˆ‘ä»¬åˆ†æ®µæ»šåŠ¨ï¼Œç¡®ä¿è§¦å‘ Lazy Load
        last_height = 0
        
        # æœ€å¤šå°è¯•æ»šåŠ¨ 20 æ¬¡ï¼Œæ¯æ¬¡æ»š 1000pxï¼Œç›´åˆ°æ»šä¸åŠ¨
        for i in range(20):
            driver.execute_script("""
                let el = (function() { 
                    let maxS = 0; let target = document.documentElement;
                    [document.documentElement, document.body, ...document.querySelectorAll('div')].forEach(e => {
                        if(e.scrollHeight > maxS && e.offsetParent !== null) { maxS = e.scrollHeight; target = e; }
                    });
                    return target;
                })();
                el.scrollTop = el.scrollHeight; 
                window.scrollTo(0, document.body.scrollHeight);
            """)
            
            time.sleep(1.5) # ç­‰å¾…åŠ è½½
            
            # æ£€æŸ¥é«˜åº¦æ˜¯å¦è¿˜åœ¨å¢é•¿
            new_height = driver.execute_script("""
                let maxS = Math.max(document.body.scrollHeight, document.documentElement.scrollHeight);
                let divs = document.querySelectorAll('div');
                for(let d of divs) { if(d.scrollHeight > maxS) maxS = d.scrollHeight; }
                return maxS;
            """)
            
            if new_height == last_height and i > 2: # è‡³å°‘æ»šä¸¤æ¬¡ç¡®è®¤
                log(f"    -> å†…å®¹åŠ è½½å®Œæ¯•ï¼Œæ£€æµ‹åˆ°é«˜åº¦: {new_height}px")
                break
            
            last_height = new_height
            if new_height > 30000:
                log("    -> é¡µé¢è¿‡é•¿ï¼Œæå‰åœæ­¢")
                break
                
        # æ»šå›é¡¶éƒ¨
        driver.execute_script("window.scrollTo(0, 0)")
        return driver, last_height

    except Exception as e:
        log(f"é¡µé¢å‡†å¤‡å¤±è´¥: {e}")
        if driver:
            driver.quit()
        return None, 0


def capture_web_screenshot_png(url, output_path):
    """
    æŠ“å–ç½‘é¡µé•¿æˆªå›¾ (PNG)
    """
    log(f"    æ­£åœ¨ç”Ÿæˆ PNG: {url} ...") 
    driver, last_height = _prepare_page_for_capture(url)
    if not driver:
        return False
        
    try:
        # æˆªå›¾: è®¾ç½®çª—å£ä¸ºæœ€å¤§æ£€æµ‹åˆ°çš„é«˜åº¦ + ç¼“å†²
        final_height = last_height + 200
        if final_height > 30000: final_height = 30000
        if final_height < 1080: final_height = 1080 # ä¿åº•
        
        log(f"Final Viewport Height: {final_height}px")
        driver.set_window_size(1920, final_height)
        time.sleep(2) # å¸ƒå±€é‡ç»˜ç­‰å¾…
        driver.save_screenshot(output_path)
        log(f"æ–‡ä»¶å·²ä¿å­˜è‡³: {output_path}")
        return True
    except Exception as e:
        log(f"PNG ç”Ÿæˆå¤±è´¥: {e}")
        return False
    finally:
        driver.quit()


def capture_web_pdf(url, output_path):
    """
    æŠ“å–ç½‘é¡µå¹¶å¯¼å‡ºä¸ºå•é¡µé•¿ PDF
    """
    log(f"    æ­£åœ¨ç”Ÿæˆ PDF: {url} ...")
    driver, last_height = _prepare_page_for_capture(url)
    if not driver:
        return False

    try:
        # PDF ä¿®å¤é€»è¾‘ï¼š
        # ç›´æ¥ä½¿ç”¨åˆšæ‰æ»šåŠ¨æ¢æµ‹åˆ°çš„çœŸå®é«˜åº¦ (last_height)
        real_height = max(last_height, 1080)
        
        driver.execute_script(f"""
            // 1. å°è¯•æ‰¾åˆ°é‚£ä¸ªæ»šåŠ¨å®¹å™¨
            let scrollEl = (function() {{ 
                let maxS = 0; let target = document.body;
                [document.documentElement, document.body, ...document.querySelectorAll('div')].forEach(e => {{
                    if(e.scrollHeight > maxS && e.offsetParent !== null) {{ maxS = e.scrollHeight; target = e; }}
                }});
                return target;
            }})();
            
            // 2. æš´åŠ›æ’‘å¼€
            let h = '{real_height}px';
            
            if(scrollEl) {{
                scrollEl.style.height = h;
                scrollEl.style.maxHeight = 'none';
                scrollEl.style.overflow = 'visible';
            }}
            document.body.style.height = h;
            document.documentElement.style.height = h;
            document.body.style.overflow = 'visible';
            document.documentElement.style.overflow = 'visible';
        """)
        time.sleep(1) # ç­‰å¾…æ¸²æŸ“æ›´æ–°

        # è®¡ç®—å°ºå¯¸
        metrics = driver.execute_script("return { width: Math.max(document.body.scrollWidth, document.documentElement.scrollWidth, 1200) }")
        page_width_in_inches = metrics['width'] / 96.0
        page_height_in_inches = (real_height + 100) / 96.0 
        
        log(f" -> ç”Ÿæˆ PDF å°ºå¯¸: {metrics['width']}x{real_height} px (ç”±æ»šåŠ¨æ¢æµ‹å†³å®š)")

        params = {
            'landscape': False,
            'displayHeaderFooter': False,
            'printBackground': True,
            'preferCSSPageSize': False,
            'paperWidth': page_width_in_inches,
            'paperHeight': page_height_in_inches,
            'marginTop': 0,
            'marginBottom': 0,
            'marginLeft': 0,
            'marginRight': 0,
        }
        
        result = driver.execute_cdp_cmd("Page.printToPDF", params)
        with open(output_path, 'wb') as f:
            f.write(base64.b64decode(result['data']))
        log(f"    æ–‡ä»¶å·²ä¿å­˜è‡³: {output_path}")
        return True
    except Exception as e:
        log(f"    PDF ç”Ÿæˆå¤±è´¥: {e}")
        return False
    finally:
        driver.quit()


# ================= ä¸»ç¨‹åºå…¥å£ =================
if __name__ == "__main__":
    final_report = "# ğŸŒ Web æƒ…æŠ¥å‘¨æŠ¥ (Automated)\n\n"
    
    for name, url in web_sources.items():
        # ç”Ÿæˆç½‘é¡µæˆªå›¾æˆ– PDF
        snapshot_path = f"data/{name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        capture_web_screenshot_png(url, snapshot_path)

        pdf_path = f"data/{name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
        capture_web_pdf(url, pdf_path)

    for name, url in web_sources.items():
        post = fetch_web_content(url)
        if post: # åªæœ‰æŠ“å–æˆåŠŸæ‰å¤„ç†
            log(f"    -> æˆåŠŸè·å–ç½‘é¡µå†…å®¹ï¼Œæ­£åœ¨æ•´ç†...")
            organized_content = organize_data([post], name)
            final_report += f"## æ¥æºï¼š{name} (Web)\n{organized_content}\n\n---\n\n"
    
    # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
    log("\n" + "="*30 + " æœ€ç»ˆæŠ¥å‘Š " + "="*30)
    log(final_report)
