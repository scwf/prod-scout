# Daft User-Defined Functions (UDFs) å®Œæ•´æŒ‡å—

Daft æä¾›äº†å¼ºå¤§çš„ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°ï¼ˆUDFï¼‰æœºåˆ¶ï¼Œå…è®¸ç”¨æˆ·çµæ´»åœ°æ‰©å±•æ•°æ®å¤„ç†èƒ½åŠ›ã€‚Daft çš„ UDF ä¸»è¦åˆ†ä¸ºä¸¤å¤§ç±»ï¼š**æ— çŠ¶æ€å‡½æ•° (`@daft.func`)** å’Œ **æœ‰çŠ¶æ€ç±» (`@daft.cls`)**ã€‚

æœ¬æ–‡æ¡£è¯¦ç»†è§£æäº† Daft ä¸­å®šä¹‰ UDF çš„å„ç§æ–¹å¼ã€é€‚ç”¨åœºæ™¯åŠä»£ç ç¤ºä¾‹ã€‚

---

## 1. æ— çŠ¶æ€å‡½æ•° (`@daft.func`)

é€‚ç”¨äºæ— éœ€ç»´æŠ¤çŠ¶æ€ã€æ— éœ€æ˜‚è´µåˆå§‹åŒ–ï¼ˆå¦‚åŠ è½½å¤§æ¨¡å‹ï¼‰çš„é€šç”¨æ•°æ®å¤„ç†é€»è¾‘ã€‚

### 1.1 Row-wise Functions (é»˜è®¤)

è¿™æ˜¯æœ€åŸºç¡€çš„ UDF å½¢å¼ï¼Œç”¨äºå¤„ç†å•è¡Œæ•°æ®ã€‚Daft ä¼šè‡ªåŠ¨å°† Python å‡½æ•°åº”ç”¨äºæ¯ä¸€è¡Œã€‚

*   **é€‚ç”¨åœºæ™¯**: é€šç”¨çš„æ•°å­¦è¿ç®—ã€å­—ç¬¦ä¸²å¤„ç†ã€ç®€å•çš„é€»è¾‘åˆ¤æ–­ã€‚
*   **ç‰¹ç‚¹**: è¾“å…¥å’Œè¾“å‡ºéƒ½æ˜¯é’ˆå¯¹å•è¡Œæ•°æ®çš„æ ‡é‡ã€‚
*   **ğŸ’¡ ç±»æ¯”**: ç›¸å½“äº Spark/Flink çš„ **`Map`** æˆ– SQL ä¸­çš„ **`Scalar UDF`**ã€‚

**ç¤ºä¾‹ä»£ç **:

```python
import daft

@daft.func
def calculate_score(priority: int, impact: float) -> float:
    """è®¡ç®—åŠ æƒåˆ†æ•°"""
    return priority * impact + 10.0

# ä½¿ç”¨ç¤ºä¾‹
df = daft.from_pydict({"priority": [1, 2, 3], "impact": [0.5, 1.5, 2.0]})
df = df.with_column("score", calculate_score(df["priority"], df["impact"]))
df.show()
```

### 1.2 Async Row-wise Functions (å¼‚æ­¥)

å½“æ‚¨çš„å¤„ç†é€»è¾‘æ¶‰åŠ I/O æ“ä½œï¼ˆå¦‚ç½‘ç»œè¯·æ±‚ã€æ•°æ®åº“æŸ¥è¯¢ï¼‰æ—¶ï¼Œä½¿ç”¨ `async` å…³é”®å­—å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚

*   **é€‚ç”¨åœºæ™¯**: è°ƒç”¨å¤–éƒ¨ APIã€æŠ“å–ç½‘é¡µå†…å®¹ã€æ•°æ®åº“æŸ¥è¯¢ç­‰ I/O å¯†é›†å‹ä»»åŠ¡ã€‚
*   **ç‰¹ç‚¹**: Daft ä¼šåˆ©ç”¨ Python çš„ Event Loop å¹¶å‘è°ƒåº¦è¿™äº›å¼‚æ­¥ä»»åŠ¡ï¼Œåœ¨å•ä¸ªçº¿ç¨‹å†…é‡å  I/O ç­‰å¾…æ—¶é—´ã€‚
*   **ğŸ’¡ ä¸ºä»€ä¹ˆéœ€è¦ Async?**: è™½ç„¶ Daft æœ¬èº«æœ‰å¹¶è¡Œï¼ˆå¤šè¿›ç¨‹/å¤šèŠ‚ç‚¹ï¼‰ï¼Œä½†æ™®é€šçš„åŒæ­¥å‡½æ•°ä¼šé˜»å¡ Worker çº¿ç¨‹ç­‰å¾… I/Oã€‚ä½¿ç”¨ `async` å¯ä»¥è®©å•ä¸ª Worker çº¿ç¨‹åœ¨ç­‰å¾…ç½‘ç»œå“åº”æ—¶å¤„ç†å…¶ä»–æ•°æ®ï¼Œæå¤§åœ°æé«˜ I/O ååé‡ã€‚

!!! warning "é£é™©æç¤ºï¼šå¹¶å‘æ§åˆ¶"
    å¦‚æœä¸åŠ é™åˆ¶ï¼ŒAsync å‡½æ•°å¯èƒ½ä¼šç¬é—´å‘èµ·æˆåƒä¸Šä¸‡ä¸ªè¯·æ±‚ï¼Œå¯¼è‡´ **IP è¢«å°** æˆ– **æœ¬åœ°è¿æ¥è€—å°½**ã€‚
    å¼ºçƒˆå»ºè®®é…åˆ **`asyncio.Semaphore`** ä½¿ç”¨æœ‰çŠ¶æ€ç±» (`@daft.cls`) æ¥æ§åˆ¶å¹¶å‘åº¦ã€‚

**ç¤ºä¾‹ä»£ç  (å¸¦å¹¶å‘æ§åˆ¶)**:

```python
import daft
import aiohttp
import asyncio

@daft.cls
class RateLimitedFetcher:
    def __init__(self, limit: int = 10):
        # é™åˆ¶è¯¥å®ä¾‹å†…éƒ¨åŒæ—¶è¿›è¡Œçš„è¯·æ±‚æ•°ä¸è¶…è¿‡ 10
        self.semaphore = asyncio.Semaphore(limit)
        
    async def __call__(self, url: str) -> str:
        async with self.semaphore:  # è·å–ä»¤ç‰Œ
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as response:
                    return await response.text()

# ä½¿ç”¨ç¤ºä¾‹
fetcher = RateLimitedFetcher(limit=10)
df = daft.from_pydict({"url": ["https://example.com/1"] * 100})
df = df.with_column("body", fetcher(df["url"]))
```

**ç®€å•ç¤ºä¾‹ (å¦‚æœä¸éœ€æ§åˆ¶å¹¶å‘)**:

**ç¤ºä¾‹ä»£ç **:

```python
import daft
import aiohttp
import asyncio

@daft.func
async def fetch_url_title(url: str) -> str:
    """å¼‚æ­¥è·å–ç½‘é¡µæ ‡é¢˜"""
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            html = await response.text()
            # ç®€å•æå– title æ ‡ç­¾ (ä¼ªä»£ç )
            return html.split("<title>")[1].split("</title>")[0]

# ä½¿ç”¨ç¤ºä¾‹
df = daft.from_pydict({"url": ["https://example.com/1", "https://example.com/2"]})
df = df.with_column("title", fetch_url_title(df["url"]))
```

### 1.3 Generator Functions (FlatMap / 1å¯¹N)

ä½¿ç”¨ `yield` å…³é”®å­—çš„ç”Ÿæˆå™¨å‡½æ•°ã€‚å®ƒå…è®¸ä¸€è¡Œè¾“å…¥äº§ç”Ÿå¤šè¡Œè¾“å‡ºï¼ˆç±»ä¼¼äº Spark/Flink ä¸­çš„ FlatMapï¼‰ã€‚

*   **é€‚ç”¨åœºæ™¯**: æ–‡æœ¬åˆ†è¯ã€å°†æ•°ç»„å±•å¼€ä¸ºå¤šè¡Œã€æ•°æ®çˆ†ç‚¸ï¼ˆExplodeï¼‰ã€‚
*   **ç‰¹ç‚¹**: è¾“å…¥ä¸€è¡Œï¼Œè¾“å‡ºé›¶è¡Œã€ä¸€è¡Œæˆ–å¤šè¡Œã€‚å…¶ä»–åˆ—çš„æ•°æ®ä¼šè‡ªåŠ¨å¤åˆ¶ï¼ˆBroadcastï¼‰ä»¥åŒ¹é…ç”Ÿæˆçš„è¡Œæ•°ã€‚
*   **ğŸ’¡ ç±»æ¯”**: ç›¸å½“äº Spark/Flink çš„ **`FlatMap`**ã€‚

**ç¤ºä¾‹ä»£ç **:

```python
import daft
from typing import Iterator

@daft.func
def split_tags(tags_str: str) -> Iterator[str]:
    """å°†é€—å·åˆ†éš”çš„æ ‡ç­¾å­—ç¬¦ä¸²æ‹†åˆ†ä¸ºå¤šè¡Œ"""
    if not tags_str:
        return
    for tag in tags_str.split(","):
        yield tag.strip()

# ä½¿ç”¨ç¤ºä¾‹
df = daft.from_pydict({
    "id": [1, 2],
    "tags": ["apple, banana", "orange"]
})
# id=1 çš„è¡Œä¼šå˜æˆä¸¤è¡Œï¼šapple å’Œ banana
df = df.select(df["id"], split_tags(df["tags"]).alias("tag"))
df.show()
```

### 1.4 Batch Functions (æ‰¹é‡/å‘é‡åŒ–)

ä½¿ç”¨ `@daft.func.batch` è£…é¥°å™¨ã€‚è¿™ç§æ–¹å¼ä¸€æ¬¡æ€§æ¥æ”¶å’Œè¿”å›ä¸€æ‰¹æ•°æ®ï¼ˆé€šå¸¸æ˜¯ `daft.Series`ï¼‰ï¼Œè€Œä¸æ˜¯å•è¡Œæ•°æ®ã€‚

*   **é€‚ç”¨åœºæ™¯**: é«˜æ€§èƒ½è®¡ç®—ï¼Œåˆ©ç”¨ NumPyã€PyArrow è¿›è¡Œå‘é‡åŒ–è¿ç®—ã€‚
*   **ç‰¹ç‚¹**: æé«˜çš„å¤„ç†é€Ÿåº¦ï¼Œé¿å…äº† Python å¾ªç¯çš„å¼€é”€ã€‚
*   **ğŸ’¡ ç±»æ¯”**: ç›¸å½“äº Spark çš„ **`MapPartitions`** æˆ– **`Pandas UDF` (Vectorized)**ã€‚

**ç¤ºä¾‹ä»£ç **:

```python
import daft
from daft import Series, DataType
import pyarrow.compute as pc

@daft.func.batch(return_dtype=DataType.int64())
def fast_add(a: Series, b: Series) -> Series:
    """ä½¿ç”¨ PyArrow è¿›è¡Œå‘é‡åŒ–åŠ æ³•"""
    # è½¬æ¢ä¸º PyArrow æ•°ç»„è¿›è¡Œè®¡ç®—
    return pc.add(a.to_arrow(), b.to_arrow())

# ä½¿ç”¨ç¤ºä¾‹
df = daft.from_pydict({"a": [1, 2, 3] * 1000, "b": [4, 5, 6] * 1000})
df = df.with_column("sum", fast_add(df["a"], df["b"]))
```

---

## 2. æœ‰çŠ¶æ€ç±» (`@daft.cls`)

é€‚ç”¨äºéœ€è¦æ˜‚è´µåˆå§‹åŒ–æ“ä½œï¼ˆInitialize-onceï¼‰çš„åœºæ™¯ã€‚Daft ä¼šåœ¨æ¯ä¸ª Worker è¿›ç¨‹ä¸Šåˆå§‹åŒ–ä¸€æ¬¡ç±»å®ä¾‹ï¼Œå¹¶åœ¨è¯¥ Worker å¤„ç†çš„æ‰€æœ‰è¡Œä¸­å¤ç”¨è¯¥å®ä¾‹ã€‚

### 2.1 Standard Stateful Class (æ ‡å‡†æœ‰çŠ¶æ€ç±»)

*   **é€‚ç”¨åœºæ™¯**: åŠ è½½æœºå™¨å­¦ä¹ æ¨¡å‹ã€å»ºç«‹æ•°æ®åº“è¿æ¥ã€åŠ è½½å¤§å‹æŸ¥æ‰¾è¡¨ã€‚
*   **ç‰¹ç‚¹**: `__init__` æ–¹æ³•åªæ‰§è¡Œä¸€æ¬¡ï¼Œ`__call__` æ–¹æ³•å¯¹æ¯ä¸€è¡Œæ‰§è¡Œã€‚

**ç¤ºä¾‹ä»£ç **:

```python
import daft

@daft.cls
class SentimentAnalyzer:
    def __init__(self, model_name: str):
        # æ˜‚è´µçš„åˆå§‹åŒ–æ“ä½œï¼šåªæ‰§è¡Œä¸€æ¬¡
        print(f"Loading model: {model_name}...")
        self.model_data = {"hello": 0.8, "bad": 0.1} # æ¨¡æ‹Ÿæ¨¡å‹

    def __call__(self, text: str) -> float:
        # å¯¹æ¯ä¸€è¡Œæ•°æ®æ‰§è¡Œ
        return self.model_data.get(text, 0.5)

# ä½¿ç”¨ç¤ºä¾‹
# åˆå§‹åŒ–æ—¶ä¼ å…¥å‚æ•°
analyzer = SentimentAnalyzer("my-bert-model")
df = daft.from_pydict({"text": ["hello", "bad", "unknown"]})
df = df.with_column("sentiment", analyzer(df["text"]))
df.show()
```

### 2.2 GPU Resource Management (GPU èµ„æºç®¡ç†)

Daft å…è®¸åœ¨å®šä¹‰ç±»æ—¶å£°æ˜æ‰€éœ€çš„èµ„æºï¼ˆå¦‚ GPUï¼‰ã€‚Daft çš„è°ƒåº¦å™¨ä¼šè‡ªåŠ¨å°†è¿™äº›ä»»åŠ¡è°ƒåº¦åˆ°æ‹¥æœ‰ GPU çš„èŠ‚ç‚¹ä¸Šã€‚

*   **é€‚ç”¨åœºæ™¯**: æ·±åº¦å­¦ä¹ æ¨ç†ï¼ˆPyTorch, TensorFlow, HuggingFaceï¼‰ã€‚
*   **é…ç½®**: ä½¿ç”¨ `gpus=N` å‚æ•°ã€‚

**ç¤ºä¾‹ä»£ç **:

```python
import daft

# å£°æ˜æ¯ä¸ªå®ä¾‹éœ€è¦ 1 ä¸ª GPU
@daft.cls(gpus=1)
class GPUClassifier:
    def __init__(self):
        # è¿™é‡Œå¯ä»¥å®‰å…¨åœ°åŠ è½½ CUDA æ¨¡å‹
        # import torch
        # self.model = torch.load("model.pth").cuda()
        pass

    def __call__(self, image_data):
        # return self.model(image_data)
        return "cat"

# ä½¿ç”¨ç¤ºä¾‹
classifier = GPUClassifier()
```

### 2.3 Batch Method (æ‰¹é‡æœ‰çŠ¶æ€æ–¹æ³•)

ç»“åˆäº†**çŠ¶æ€ä¿æŒ**å’Œ**æ‰¹é‡å¤„ç†**çš„ä¼˜åŠ¿ã€‚è¿™å¯¹äºç°ä»£æ·±åº¦å­¦ä¹ æ¨ç†è‡³å…³é‡è¦ï¼Œå› ä¸ºæ‰¹é‡æ¨ç†ï¼ˆBatch Inferenceï¼‰é€šå¸¸æ¯”å•æ¡æ¨ç†å¿«å¾—å¤šã€‚

*   **é€‚ç”¨åœºæ™¯**: é«˜ååé‡çš„ AI æ¨¡å‹æ¨ç†ã€‚
*   **ç‰¹ç‚¹**: ä½¿ç”¨ `@daft.method.batch` è£…é¥°ç±»æ–¹æ³•ã€‚

**ç¤ºä¾‹ä»£ç **:

```python
import daft
from daft import Series, DataType

@daft.cls
class BatchInferenceModel:
    def __init__(self):
        # self.model = load_model()
        pass

    @daft.method.batch(return_dtype=DataType.string())
    def predict_batch(self, features: Series) -> Series:
        # å°†è¾“å…¥åºåˆ—è½¬æ¢ä¸ºé€‚åˆæ¨¡å‹è¾“å…¥çš„æ‰¹é‡æ ¼å¼ (å¦‚ numpy array)
        numpy_batch = features.to_arrow().to_numpy()
        
        # æ‰§è¡Œæ‰¹é‡æ¨ç†
        # options = self.model.predict(numpy_batch)
        results = [f"pred_{x}" for x in numpy_batch] # æ¨¡æ‹Ÿç»“æœ
        
        return Series.from_pylist(results)

# ä½¿ç”¨ç¤ºä¾‹
model = BatchInferenceModel()
df = daft.from_pydict({"features": [1, 2, 3, 4]})
# æ³¨æ„è°ƒç”¨çš„æ˜¯ predict_batch æ–¹æ³•
df = df.with_column("prediction", model.predict_batch(df["features"]))
```

---

## 3. å…¶ä»–å®šä¹‰æ–¹å¼

### 3.1 `.apply()`

ç±»ä¼¼äº Pandas çš„ applyã€‚

*   **é€‚ç”¨åœºæ™¯**: å¿«é€ŸåŸå‹å¼€å‘ï¼Œéå¸¸ç®€å•çš„å•åˆ—å¤„ç†ã€‚
*   **é™åˆ¶**: æ€§èƒ½è¾ƒå·®ï¼ˆçº¯ Python å¾ªç¯ï¼‰ï¼Œæ— æ³•å¤„ç†å¤šåˆ—è¾“å…¥ã€‚

```python
df.with_column("new_col", df["old_col"].apply(lambda x: x * 2))
```

### 3.2 Legacy `@daft.udf`

Daft çš„æ—§ç‰ˆ APIã€‚

*   **æ³¨æ„**: åŠŸèƒ½å·²è¢«ä¸Šè¿°çš„æ–° API (`@daft.func`, `@daft.cls`) è¦†ç›–ã€‚å»ºè®®åœ¨æ–°ä»£ç ä¸­é¿å…ä½¿ç”¨ï¼Œä»¥ç¡®ä¿æœªæ¥çš„å…¼å®¹æ€§ã€‚

---

## æ€»ç»“ cheat sheet

| éœ€æ±‚ | æ¨èæ–¹æ³• | å…³é”®è£…é¥°å™¨/æ–¹æ³• |
| :--- | :--- | :--- |
| **é€šç”¨ç®€å•é€»è¾‘** | Stateless Function | `@daft.func` |
| **API è°ƒç”¨ / çˆ¬è™«** | Async Function | `@daft.func` (on `async def`) |
| **æ–‡æœ¬åˆ†è¯ / å±•å¼€** | Generator Function | `@daft.func` (with `yield`) |
| **é«˜æ€§èƒ½æ•°å€¼è®¡ç®—** | Batch Function | `@daft.func.batch` |
| **AI æ¨¡å‹æ¨ç† (CPU/GPU)** | Stateful Class | `@daft.cls` |
| **æ‰¹é‡ AI æ¨ç†** | Batch Method | `@daft.cls` + `@daft.method.batch` |

## é™„å½•ï¼šå¤§æ•°æ®æ¡†æ¶æ¦‚å¿µæ˜ å°„è¡¨

å¦‚æœæ‚¨ç†Ÿæ‚‰ Spark æˆ– Flinkï¼Œå¯ä»¥å‚è€ƒä¸‹è¡¨å¿«é€Ÿç†è§£ Daft çš„æ¦‚å¿µï¼š

| Daft UDF ç±»å‹ | Spark å¯¹åº”æ¦‚å¿µ | Flink å¯¹åº”æ¦‚å¿µ | è¾“å…¥->è¾“å‡º |
| :--- | :--- | :--- | :--- |
| **Row-wise (`@daft.func`)** | `map()` / `udf()` | `map()` / Scalar Function | 1 -> 1 |
| **Generator (`@daft.func` yield)** | `flatMap()` / `explode()` | `flatMap()` / Table Function | 1 -> N |
| **Batch (`@daft.func.batch`)** | `mapPartitions()` / Pandas UDF | - | 1 Batch -> 1 Batch |
| **Stateful Class (`@daft.cls`)** | (æ— ç›´æ¥å¯¹åº”ï¼Œéœ€åœ¨ `mapPartitions` æ‰‹åŠ¨å®ç°) | RichFunction (`open()` æ–¹æ³•) | (æœ‰çŠ¶æ€å¤ç”¨) |
