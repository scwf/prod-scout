"""
scraper.py - X/Twitter ç”¨æˆ·æ¨æ–‡çˆ¬å–ç¼–æ’å™¨

é¢å‘ç”¨æˆ·çš„é«˜å±‚ APIï¼Œæ•´åˆ XClientã€AccountPool å’Œé…ç½®ç®¡ç†ã€‚
æ”¯æŒï¼š
- ç‹¬ç«‹è¿è¡Œ (CLI æ¨¡å¼)
- é›†æˆåˆ° Pipeline (ä½œä¸º FetcherStage çš„æ•°æ®æº)
"""
import os
import sys
import json
import time
import random
import logging
import configparser
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Optional, Tuple, Any

from .client import XClient
from .account_pool import AccountPool
from .models import Tweet

logger = logging.getLogger("x_scraper.scraper")


class XScraper:
    """
    X ç”¨æˆ·æ¨æ–‡çˆ¬å–å™¨ (High-level API)

    ç”¨æ³•:
        # ä» config.ini åˆ›å»º
        scraper = XScraper.from_config(config)

        # æŠ“å–å•ä¸ªç”¨æˆ·
        tweets = scraper.fetch_user_tweets("karpathy", limit=20)

        # æ‰¹é‡æŠ“å–é…ç½®ä¸­çš„æ‰€æœ‰ç”¨æˆ·
        all_posts = scraper.fetch_all_configured_users(days_lookback=7)
    """

    def __init__(
        self,
        account_pool: AccountPool,
        max_tweets_per_user: int = 20,
        request_delay: Tuple[float, float] = (15.0, 25.0),
        user_switch_delay: Tuple[float, float] = (30.0, 60.0),
        request_timeout: int = 30,
        max_retries: int = 3,
        include_retweets: bool = False,
        include_replies: bool = False,
        circuit_breaker_threshold: int = 5,
        circuit_breaker_cooldown: int = 60,
        query_ids: Optional[Dict[str, str]] = None,
        features: Optional[Dict[str, Any]] = None,
    ):
        """
        åˆå§‹åŒ– X çˆ¬å–å™¨ã€‚

        Args:
            account_pool: è´¦å·æ± å®ä¾‹
            max_tweets_per_user: æ¯ä¸ªç”¨æˆ·æŠ“å–çš„æ¨æ–‡ä¸Šé™
            request_delay: è¯·æ±‚é—´å»¶è¿ŸèŒƒå›´ (ç§’)
            user_switch_delay: ç”¨æˆ·åˆ‡æ¢é—´å»¶è¿ŸèŒƒå›´ (ç§’)
            request_timeout: è¯·æ±‚è¶…æ—¶ (ç§’)
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            include_retweets: æ˜¯å¦åŒ…å«è½¬æ¨
            include_replies: æ˜¯å¦åŒ…å«å›å¤
            circuit_breaker_threshold: æ–­è·¯å™¨é˜ˆå€¼ (è¿ç»­å¤±è´¥æ¬¡æ•°)
            circuit_breaker_cooldown: æ–­è·¯å™¨å†·å´æ—¶é—´ (ç§’)
            query_ids: è‡ªå®šä¹‰ GraphQL Query IDs
            features: è‡ªå®šä¹‰ GraphQL Features
        """
        self.account_pool = account_pool
        self.max_tweets_per_user = max_tweets_per_user
        self.request_delay = request_delay
        self.user_switch_delay = user_switch_delay
        self.include_retweets = include_retweets
        self.include_replies = include_replies

        self.client = XClient(
            account_pool=account_pool,
            timeout=request_timeout,
            max_retries=max_retries,
            circuit_breaker_threshold=circuit_breaker_threshold,
            circuit_breaker_cooldown=circuit_breaker_cooldown,
            query_ids=query_ids,
            features=features,
        )

    @classmethod
    def from_config(cls, config: configparser.ConfigParser) -> 'XScraper':
        """
        ä» config.ini é…ç½®åˆ›å»º XScraper å®ä¾‹ã€‚

        è¯»å– [x_scraper] èŠ‚çš„é…ç½®å‚æ•°ã€‚
        ä¼˜å…ˆä» [x_scraper] auth_credentials è¯»å–å‡­è¯ï¼Œ
        å¦‚æ‰¾ä¸åˆ°åˆ™å°è¯•ä» rsshub-docker.env ä¸­è¯»å–ã€‚

        Args:
            config: ConfigParser å®ä¾‹

        Returns:
            XScraper å®ä¾‹
        """
        # â”€â”€â”€ åŠ è½½è´¦å·å‡­è¯ â”€â”€â”€
        auth_str = config.get('x_scraper', 'auth_credentials', fallback='').strip()

        if auth_str:
            pool = AccountPool.from_config_string(auth_str)
        else:
            # å›é€€: å°è¯•ä» rsshub-docker.env åŠ è½½
            project_root = _find_project_root()
            env_files = [
                os.path.join(project_root, "rsshub-docker.env"),
            ]
            pool = None
            for env_file in env_files:
                if os.path.exists(env_file):
                    try:
                        pool = AccountPool.from_env_file(env_file)
                        logger.info(f"ä» {os.path.basename(env_file)} åŠ è½½å‡­è¯")
                        break
                    except Exception as e:
                        logger.warning(f"åŠ è½½ {env_file} å¤±è´¥: {e}")

            if pool is None:
                raise ValueError(
                    "æœªæ‰¾åˆ° X è´¦å·å‡­è¯ã€‚è¯·åœ¨ config.ini [x_scraper] ä¸­é…ç½® auth_credentialsï¼Œ"
                    "æˆ–ç¡®ä¿ rsshub-docker.env æ–‡ä»¶å­˜åœ¨ã€‚"
                )

        # â”€â”€â”€ è¯»å–å…¶ä»–é…ç½® â”€â”€â”€
        # P2: åŠ è½½å¯é…ç½®çš„ Query IDs å’Œ Features (è¦†ç›–ä»£ç ä¸­çš„é»˜è®¤å€¼)
        query_ids = None
        features = None
        query_ids_str = config.get('x_scraper', 'query_ids', fallback='').strip()
        features_str = config.get('x_scraper', 'features', fallback='').strip()
        if query_ids_str:
            try:
                query_ids = json.loads(query_ids_str)
                logger.info(f"ä»é…ç½®åŠ è½½è‡ªå®šä¹‰ Query IDs: {list(query_ids.keys())}")
            except json.JSONDecodeError as e:
                logger.warning(f"è§£æ query_ids é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        if features_str:
            try:
                features = json.loads(features_str)
                logger.info(f"ä»é…ç½®åŠ è½½è‡ªå®šä¹‰ Features ({len(features)} ä¸ª)")
            except json.JSONDecodeError as e:
                logger.warning(f"è§£æ features é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼")

        return cls(
            account_pool=pool,
            max_tweets_per_user=config.getint('x_scraper', 'max_tweets_per_user', fallback=20),
            request_delay=(
                config.getfloat('x_scraper', 'request_delay_min', fallback=15.0),
                config.getfloat('x_scraper', 'request_delay_max', fallback=25.0),
            ),
            user_switch_delay=(
                config.getfloat('x_scraper', 'user_switch_delay_min', fallback=30.0),
                config.getfloat('x_scraper', 'user_switch_delay_max', fallback=60.0),
            ),
            request_timeout=config.getint('x_scraper', 'request_timeout', fallback=30),
            max_retries=config.getint('x_scraper', 'max_retries', fallback=3),
            include_retweets=config.getboolean('x_scraper', 'include_retweets', fallback=False),
            include_replies=config.getboolean('x_scraper', 'include_replies', fallback=False),
            # P1 & P2: æ–°å¢å‚æ•°
            circuit_breaker_threshold=config.getint('x_scraper', 'circuit_breaker_threshold', fallback=5),
            circuit_breaker_cooldown=config.getint('x_scraper', 'circuit_breaker_cooldown', fallback=60),
            query_ids=query_ids,
            features=features,
        )

    # â”€â”€â”€ æ ¸å¿ƒ API â”€â”€â”€

    def fetch_user_tweets(
        self,
        username: str,
        limit: Optional[int] = None,
        days_lookback: Optional[int] = None,
    ) -> List[Tweet]:
        """
        æŠ“å–å•ä¸ªç”¨æˆ·çš„æ¨æ–‡ã€‚

        Args:
            username: X ç”¨æˆ·å (ä¸å« @)
            limit: æ¨æ–‡æ•°é‡ä¸Šé™ (é»˜è®¤ä½¿ç”¨é…ç½®å€¼)
            days_lookback: å›æº¯å¤©æ•° (é»˜è®¤ä¸é™)

        Returns:
            Tweet å¯¹è±¡åˆ—è¡¨
        """
        if limit is None:
            limit = self.max_tweets_per_user

        # 1. è·å– user_id
        logger.info(f"ğŸ”„ [X Scraper] è·å–ç”¨æˆ· @{username} çš„ ID...")
        user_id = self.client.get_user_id(username)
        if not user_id:
            logger.warning(f"æ— æ³•è·å–ç”¨æˆ· @{username} çš„ IDï¼Œè·³è¿‡")
            return []

        # 2. è®¡ç®—æ—¥æœŸæˆªæ­¢
        since_date = None
        if days_lookback:
            cutoff = datetime.now(timezone.utc) - timedelta(days=days_lookback)
            since_date = cutoff.strftime("%Y-%m-%d")

        # 3. è·å–æ¨æ–‡
        logger.info(f"ğŸ”„ [X Scraper] æŠ“å– @{username} çš„æ¨æ–‡ (limit={limit}, since={since_date})...")
        tweets = self.client.get_user_tweets_all(
            user_id=user_id,
            limit=limit,
            since_date=since_date,
            include_replies=self.include_replies,
            include_retweets=self.include_retweets,
            page_delay=self.request_delay,
        )

        logger.info(f"âœ… [X Scraper] @{username}: è·å–åˆ° {len(tweets)} æ¡æ¨æ–‡")
        return tweets

    def fetch_user_tweets_as_posts(
        self,
        username: str,
        source_name: str,
        limit: Optional[int] = None,
        days_lookback: Optional[int] = None,
    ) -> List[dict]:
        """
        æŠ“å–æ¨æ–‡å¹¶è½¬æ¢ä¸º Pipeline å…¼å®¹çš„ post dict åˆ—è¡¨ã€‚

        è¿™æ˜¯é›†æˆåˆ° Pipeline FetcherStage çš„ä¸»è¦å…¥å£ã€‚

        Args:
            username: X ç”¨æˆ·å
            source_name: æºåç§° (å¦‚ "X_OpenAI")
            limit: æ¨æ–‡ä¸Šé™
            days_lookback: å›æº¯å¤©æ•°

        Returns:
            Pipeline å…¼å®¹çš„ post dict åˆ—è¡¨
        """
        tweets = self.fetch_user_tweets(username, limit, days_lookback)
        return [tweet.to_post_dict(source_name) for tweet in tweets]

    def fetch_all_configured_users(
        self,
        x_accounts: Dict[str, str],
        days_lookback: int = 7,
    ) -> Dict[str, List[dict]]:
        """
        æ‰¹é‡æŠ“å–é…ç½®ä¸­çš„æ‰€æœ‰ X ç”¨æˆ·ã€‚

        Args:
            x_accounts: {source_name: username} å­—å…¸ (æ¥è‡ª config.ini [x_accounts])
            days_lookback: å›æº¯å¤©æ•°

        Returns:
            {source_name: [post_dict, ...]} å­—å…¸
        """
        results = {}
        total = len(x_accounts)
        logger.info(f"â”â”â” X Scraper: å¼€å§‹æ‰¹é‡æŠ“å– {total} ä¸ªç”¨æˆ· â”â”â”")

        for i, (source_name, username) in enumerate(x_accounts.items(), 1):
            logger.info(f"[{i}/{total}] å¤„ç† {source_name} (@{username})...")

            try:
                posts = self.fetch_user_tweets_as_posts(
                    username=username,
                    source_name=source_name,
                    days_lookback=days_lookback,
                )
                results[source_name] = posts

            except Exception as e:
                logger.error(f"æŠ“å– @{username} å¤±è´¥: {e}")
                results[source_name] = []

            # ç”¨æˆ·é—´å»¶è¿Ÿ (æœ€åä¸€ä¸ªç”¨æˆ·ä¸éœ€è¦)
            if i < total:
                delay = random.uniform(*self.user_switch_delay)
                logger.info(f"â³ ç”¨æˆ·åˆ‡æ¢å»¶è¿Ÿ {delay:.1f}s...")
                time.sleep(delay)

        # ç»Ÿè®¡
        total_posts = sum(len(v) for v in results.values())
        success_count = sum(1 for v in results.values() if v)
        logger.info(
            f"â”â”â” X Scraper å®Œæˆ: {success_count}/{total} ä¸ªç”¨æˆ·æˆåŠŸ, "
            f"å…± {total_posts} æ¡æ¨æ–‡ â”â”â”"
        )

        return results


# â”€â”€â”€ è¾…åŠ©å‡½æ•° â”€â”€â”€

def _find_project_root() -> str:
    """æŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½• (åŒ…å« config.ini çš„ç›®å½•)"""
    # ä»å½“å‰æ–‡ä»¶å‘ä¸ŠæŸ¥æ‰¾
    current = os.path.dirname(os.path.abspath(__file__))
    for _ in range(5):  # æœ€å¤šå‘ä¸Š 5 å±‚
        if os.path.exists(os.path.join(current, "config.ini")):
            return current
        current = os.path.dirname(current)
    # å›é€€åˆ°å½“å‰å·¥ä½œç›®å½•
    return os.getcwd()


def _load_config() -> configparser.ConfigParser:
    """åŠ è½½ config.ini"""
    config = configparser.ConfigParser()
    config.optionxform = str  # ä¿æŒ key å¤§å°å†™
    project_root = _find_project_root()
    config_path = os.path.join(project_root, "config.ini")
    config.read(config_path, encoding='utf-8')
    return config


def _load_x_accounts(config: configparser.ConfigParser) -> Dict[str, str]:
    """ä» config.ini åŠ è½½ X è´¦å·åˆ—è¡¨"""
    accounts = {}
    if config.has_section('x_accounts'):
        for name in config.options('x_accounts'):
            username = config.get('x_accounts', name).strip()
            if username:
                accounts[name] = username
    return accounts


# â”€â”€â”€ CLI å…¥å£ â”€â”€â”€

def main():
    """
    ç‹¬ç«‹è¿è¡Œå…¥å£ã€‚

    è¯»å– config.ini ä¸­çš„ [x_accounts] å’Œ [x_scraper] é…ç½®ï¼Œ
    æŠ“å–æ‰€æœ‰ç”¨æˆ·çš„æ¨æ–‡å¹¶ä¿å­˜åˆ° data/ ç›®å½•ã€‚
    """
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    )

    # åŠ è½½é…ç½®
    config = _load_config()
    x_accounts = _load_x_accounts(config)

    if not x_accounts:
        logger.error("config.ini ä¸­æœªæ‰¾åˆ° [x_accounts] é…ç½®")
        sys.exit(1)

    logger.info(f"åŠ è½½äº† {len(x_accounts)} ä¸ª X è´¦å·")

    # åˆ›å»º scraper
    try:
        scraper = XScraper.from_config(config)
    except ValueError as e:
        logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
        sys.exit(1)

    # è·å–å›æº¯å¤©æ•°
    days_lookback = config.getint('crawler', 'days_lookback', fallback=7)

    # æ‰§è¡ŒæŠ“å–
    results = scraper.fetch_all_configured_users(x_accounts, days_lookback=days_lookback)

    # ä¿å­˜ç»“æœ
    batch_ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    project_root = _find_project_root()
    output_dir = os.path.join(project_root, 'data', f'x_scraper_{batch_ts}')
    os.makedirs(output_dir, exist_ok=True)

    for source_name, posts in results.items():
        if posts:
            filepath = os.path.join(output_dir, f"{source_name}.json")
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(posts, f, ensure_ascii=False, indent=2)

    total_posts = sum(len(v) for v in results.values())
    logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_dir} (å…± {total_posts} æ¡æ¨æ–‡)")


if __name__ == "__main__":
    main()
